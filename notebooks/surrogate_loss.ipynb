{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import einops as ein"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ParameterizedProb(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ParameterizedProb, self).__init__()\n",
    "        self.logits = nn.Parameter(torch.rand(7, 10))\n",
    "\n",
    "    def f(self, samples):\n",
    "        \"\"\"\n",
    "        A function of individual samples\n",
    "        \"\"\"\n",
    "        return torch.pow(samples, 2).sum(dim=-1)\n",
    "\n",
    "    def forward(self):\n",
    "        \"\"\"\n",
    "        Samples of x composed with f, where < f(x) > is being minimized, along with the logs of\n",
    "        probabilities associated with sampling each x\n",
    "        \"\"\"\n",
    "\n",
    "        # A probability distribution that's a function of model params\n",
    "        cat = torch.distributions.Categorical(logits=self.logits)\n",
    "\n",
    "        samples = cat.sample((100,))  # (100, 7)\n",
    "        log_probs = cat.log_prob(samples)  # (100, 7)\n",
    "        samples = samples.to(dtype=torch.float32)\n",
    "\n",
    "        # Log probs of sampling each x (sum over log probs of the entries of each x)\n",
    "        log_probs = ein.reduce(\n",
    "            log_probs,\n",
    "            \"b s -> b\",\n",
    "            reduction=\"sum\",\n",
    "        )\n",
    "\n",
    "        assert torch.all(\n",
    "            torch.exp(log_probs) >= 0.0\n",
    "        ), \"Probs over whole chains must be geq0\"\n",
    "        assert torch.all(\n",
    "            torch.exp(log_probs) <= 1.0\n",
    "        ), \"Probs over whole chains must be leq 1\"\n",
    "\n",
    "        f_values = self.f(samples)  # (100,)\n",
    "        return f_values, log_probs\n",
    "\n",
    "    def surrogate_loss(self, f_values, log_probs):\n",
    "        loss = ein.einsum(\n",
    "            f_values,\n",
    "            log_probs,\n",
    "            \"b, b -> \",\n",
    "        )\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([100])"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p = ParameterizedProb()\n",
    "f, log_probs = p.forward()\n",
    "f.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(-288449.0938, grad_fn=<ViewBackward0>)"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p.surrogate_loss(f, log_probs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This does "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 100/1000, Loss: 0.0 f: 0.0 log_probs: -0.0015707015991210938\n",
      "Epoch 200/1000, Loss: 0.0 f: 0.0 log_probs: -0.0015802383422851562\n",
      "Epoch 300/1000, Loss: 0.0 f: 0.0 log_probs: -0.0015869140625\n",
      "Epoch 400/1000, Loss: 0.0 f: 0.0 log_probs: -0.0015802383422851562\n",
      "Epoch 500/1000, Loss: 0.0 f: 0.0 log_probs: -0.0015716552734375\n",
      "Epoch 600/1000, Loss: 0.0 f: 0.0 log_probs: -0.0015687942504882812\n",
      "Epoch 700/1000, Loss: 0.0 f: 0.0 log_probs: -0.001560211181640625\n",
      "Epoch 800/1000, Loss: -9.958617210388184 f: 0.009999999776482582 log_probs: -0.10111095756292343\n",
      "Epoch 900/1000, Loss: 0.0 f: 0.0 log_probs: -0.0015306472778320312\n",
      "Epoch 1000/1000, Loss: 0.0 f: 0.0 log_probs: -0.0015077590942382812\n"
     ]
    }
   ],
   "source": [
    "# Define optimizer\n",
    "optimizer = torch.optim.Adam(p.parameters(), lr=1e-3)\n",
    "\n",
    "num_epochs = 1000\n",
    "for epoch in range(num_epochs):\n",
    "    optimizer.zero_grad()  # Clear gradients\n",
    "\n",
    "    f_values, log_probs = p.forward()\n",
    "    loss = p.surrogate_loss(f_values, log_probs)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    if (epoch + 1) % 100 == 0:\n",
    "        print(\n",
    "            f\"Epoch {epoch + 1}/{num_epochs}, Loss: {loss.item()} f: {f_values.mean().item()} log_probs: {log_probs.mean().item()}\"\n",
    "        )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
