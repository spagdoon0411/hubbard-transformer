{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import einops as ein"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[7, 0, 4, 0, 9, 6, 3],\n",
       "        [4, 3, 5, 2, 1, 6, 4],\n",
       "        [3, 6, 8, 3, 5, 1, 9],\n",
       "        [0, 6, 9, 2, 3, 5, 7],\n",
       "        [0, 7, 6, 5, 2, 4, 8],\n",
       "        [5, 4, 4, 7, 0, 1, 3],\n",
       "        [0, 8, 4, 2, 8, 3, 3],\n",
       "        [1, 5, 1, 5, 9, 9, 1],\n",
       "        [2, 1, 7, 4, 7, 7, 8],\n",
       "        [1, 3, 0, 6, 2, 9, 0],\n",
       "        [0, 8, 2, 0, 3, 5, 7],\n",
       "        [3, 4, 0, 3, 5, 3, 7],\n",
       "        [6, 7, 9, 5, 6, 8, 3],\n",
       "        [7, 6, 1, 9, 1, 3, 7],\n",
       "        [8, 7, 0, 0, 2, 8, 1],\n",
       "        [7, 3, 2, 6, 6, 3, 8],\n",
       "        [6, 4, 9, 6, 8, 2, 3],\n",
       "        [0, 2, 9, 5, 3, 7, 8],\n",
       "        [5, 0, 4, 5, 0, 2, 7],\n",
       "        [8, 8, 9, 4, 1, 0, 9],\n",
       "        [8, 4, 9, 0, 4, 7, 3],\n",
       "        [3, 1, 7, 4, 6, 7, 2],\n",
       "        [4, 2, 0, 2, 4, 5, 6],\n",
       "        [3, 2, 5, 1, 3, 9, 6],\n",
       "        [2, 6, 1, 2, 2, 3, 3],\n",
       "        [4, 4, 1, 9, 8, 6, 3],\n",
       "        [0, 6, 9, 1, 3, 2, 4],\n",
       "        [8, 6, 4, 0, 1, 1, 6],\n",
       "        [3, 4, 7, 5, 0, 6, 3],\n",
       "        [8, 2, 2, 9, 7, 7, 5],\n",
       "        [0, 3, 6, 4, 4, 8, 6],\n",
       "        [2, 5, 3, 1, 9, 5, 8],\n",
       "        [7, 2, 6, 6, 9, 1, 7],\n",
       "        [8, 8, 4, 4, 2, 8, 8],\n",
       "        [4, 7, 0, 1, 0, 6, 0],\n",
       "        [7, 6, 6, 5, 7, 7, 8],\n",
       "        [0, 0, 0, 2, 0, 7, 5],\n",
       "        [3, 3, 5, 0, 4, 5, 6],\n",
       "        [7, 7, 6, 0, 6, 2, 6],\n",
       "        [2, 7, 2, 3, 8, 7, 8],\n",
       "        [7, 4, 3, 5, 5, 7, 1],\n",
       "        [4, 5, 9, 2, 0, 8, 4],\n",
       "        [0, 0, 6, 5, 3, 8, 4],\n",
       "        [5, 6, 3, 0, 4, 0, 8],\n",
       "        [2, 9, 7, 2, 0, 3, 0],\n",
       "        [1, 4, 0, 5, 0, 6, 1],\n",
       "        [0, 6, 7, 4, 0, 9, 7],\n",
       "        [8, 9, 6, 3, 3, 7, 8],\n",
       "        [2, 1, 6, 6, 2, 1, 1],\n",
       "        [3, 5, 6, 4, 4, 4, 7],\n",
       "        [4, 8, 5, 7, 0, 8, 8],\n",
       "        [2, 8, 8, 4, 0, 8, 6],\n",
       "        [0, 9, 1, 5, 9, 2, 1],\n",
       "        [0, 8, 2, 4, 2, 6, 7],\n",
       "        [2, 7, 2, 5, 7, 3, 4],\n",
       "        [0, 7, 1, 2, 0, 6, 5],\n",
       "        [4, 9, 3, 5, 5, 1, 0],\n",
       "        [8, 8, 8, 2, 2, 2, 2],\n",
       "        [8, 5, 9, 6, 8, 7, 5],\n",
       "        [5, 8, 6, 2, 0, 0, 7],\n",
       "        [4, 8, 5, 0, 6, 0, 8],\n",
       "        [5, 9, 3, 2, 6, 5, 5],\n",
       "        [0, 1, 8, 7, 8, 9, 3],\n",
       "        [0, 7, 9, 6, 3, 3, 7],\n",
       "        [5, 4, 0, 5, 6, 3, 4],\n",
       "        [2, 6, 5, 0, 9, 4, 1],\n",
       "        [3, 7, 7, 4, 0, 8, 8],\n",
       "        [4, 5, 6, 7, 5, 8, 4],\n",
       "        [5, 5, 6, 1, 2, 6, 0],\n",
       "        [4, 3, 1, 4, 6, 6, 0],\n",
       "        [7, 4, 4, 1, 0, 3, 0],\n",
       "        [0, 8, 1, 7, 6, 7, 1],\n",
       "        [4, 2, 5, 6, 3, 5, 3],\n",
       "        [4, 2, 3, 4, 4, 5, 5],\n",
       "        [8, 0, 9, 5, 8, 1, 1],\n",
       "        [7, 8, 6, 7, 0, 3, 7],\n",
       "        [8, 6, 8, 4, 3, 6, 6],\n",
       "        [1, 9, 6, 4, 8, 0, 0],\n",
       "        [8, 4, 9, 0, 4, 4, 7],\n",
       "        [6, 9, 1, 4, 4, 3, 3],\n",
       "        [1, 2, 1, 0, 6, 5, 5],\n",
       "        [7, 8, 7, 5, 1, 3, 6],\n",
       "        [8, 7, 8, 0, 0, 4, 7],\n",
       "        [3, 5, 1, 5, 5, 3, 7],\n",
       "        [8, 3, 0, 3, 8, 5, 8],\n",
       "        [2, 1, 6, 8, 0, 8, 1],\n",
       "        [6, 4, 9, 0, 0, 1, 3],\n",
       "        [2, 3, 6, 6, 6, 4, 0],\n",
       "        [5, 1, 9, 1, 7, 3, 6],\n",
       "        [6, 6, 0, 9, 2, 2, 3],\n",
       "        [5, 2, 9, 9, 4, 0, 8],\n",
       "        [2, 6, 0, 9, 0, 3, 9],\n",
       "        [0, 4, 0, 9, 6, 2, 3],\n",
       "        [9, 0, 3, 1, 3, 3, 5],\n",
       "        [2, 4, 2, 7, 0, 7, 1],\n",
       "        [8, 2, 9, 5, 2, 3, 8],\n",
       "        [7, 5, 5, 6, 9, 4, 4],\n",
       "        [9, 4, 6, 6, 7, 1, 1],\n",
       "        [5, 7, 1, 4, 0, 5, 8],\n",
       "        [6, 7, 9, 5, 2, 2, 4]])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logits = nn.Parameter(torch.rand(7, 10))\n",
    "# Log of the probability mass function evaluated at some value?\n",
    "cat = torch.distributions.Categorical(logits=logits)\n",
    "test_samples = cat.sample((100,))\n",
    "test_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ParameterizedProb(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ParameterizedProb, self).__init__()\n",
    "        self.logits = nn.Parameter(torch.rand(7, 10))\n",
    "\n",
    "    def f(self, samples):\n",
    "        \"\"\"\n",
    "        A function of individual samples, which are 7 long.\n",
    "        \"\"\"\n",
    "        return -torch.pow(samples, 2).sum(dim=-1)\n",
    "\n",
    "    def forward(self):\n",
    "        \"\"\"\n",
    "        Samples of x composed with f, where < f(x) > is being minimized, along with the logs of\n",
    "        probabilities associated with sampling each x\n",
    "        \"\"\"\n",
    "\n",
    "        # A probability distribution that's a function of model params\n",
    "        cat = torch.distributions.Categorical(logits=self.logits)\n",
    "\n",
    "        samples = cat.sample((100,))  # (100, 7)\n",
    "        log_probs = cat.log_prob(samples)  # (100, 7)\n",
    "        samples = samples.to(dtype=torch.float32)\n",
    "\n",
    "        # The class dimension is the last dimension, not the first\n",
    "\n",
    "        # Log probs of sampling each x (sum over log probs of the entries of each x)\n",
    "        log_probs = ein.reduce(\n",
    "            log_probs,\n",
    "            \"b s -> b\",\n",
    "            reduction=\"sum\",\n",
    "        )\n",
    "\n",
    "        assert torch.all(torch.exp(log_probs) >= 0.0), (\n",
    "            \"Probs over whole chains must be geq0\"\n",
    "        )\n",
    "        assert torch.all(torch.exp(log_probs) <= 1.0), (\n",
    "            \"Probs over whole chains must be leq 1\"\n",
    "        )\n",
    "\n",
    "        f_values = self.f(samples)  # (100,)\n",
    "        return f_values, log_probs\n",
    "\n",
    "    def surrogate_loss(self, f_values, log_probs):\n",
    "        loss = ein.einsum(\n",
    "            f_values,\n",
    "            log_probs,\n",
    "            \"b, b -> \",\n",
    "        )\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([100])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p = ParameterizedProb()\n",
    "f, log_probs = p.forward()\n",
    "f.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(321283.8125, grad_fn=<ViewBackward0>)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p.surrogate_loss(f, log_probs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This does seem to minimize loss with similar surrogate loss behavior (becoming less negative per backprop iteration):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 100/100000, Loss: 313324.25 f: -197.3699951171875 log_probs: -15.908193588256836\n",
      "Epoch 200/100000, Loss: 357144.40625 f: -225.33999633789062 log_probs: -15.941043853759766\n",
      "Epoch 300/100000, Loss: 334820.90625 f: -210.9499969482422 log_probs: -15.913761138916016\n",
      "Epoch 400/100000, Loss: 364514.25 f: -231.44000244140625 log_probs: -15.875053405761719\n",
      "Epoch 500/100000, Loss: 357564.0625 f: -229.61000061035156 log_probs: -15.693944931030273\n",
      "Epoch 600/100000, Loss: 363309.75 f: -232.2100067138672 log_probs: -15.908476829528809\n",
      "Epoch 700/100000, Loss: 382260.625 f: -247.75999450683594 log_probs: -15.595168113708496\n",
      "Epoch 800/100000, Loss: 396765.75 f: -258.239990234375 log_probs: -15.527678489685059\n",
      "Epoch 900/100000, Loss: 415146.8125 f: -270.5799865722656 log_probs: -15.559765815734863\n",
      "Epoch 1000/100000, Loss: 390566.0 f: -255.5399932861328 log_probs: -15.574394226074219\n",
      "Epoch 1100/100000, Loss: 424035.5 f: -284.489990234375 log_probs: -15.249031066894531\n",
      "Epoch 1200/100000, Loss: 420639.46875 f: -280.67999267578125 log_probs: -15.248237609863281\n",
      "Epoch 1300/100000, Loss: 422542.34375 f: -282.54998779296875 log_probs: -15.275836944580078\n",
      "Epoch 1400/100000, Loss: 436719.65625 f: -298.8900146484375 log_probs: -14.954768180847168\n",
      "Epoch 1500/100000, Loss: 443862.125 f: -305.3599853515625 log_probs: -14.884530067443848\n",
      "Epoch 1600/100000, Loss: 446236.09375 f: -309.239990234375 log_probs: -14.7576265335083\n",
      "Epoch 1700/100000, Loss: 455224.6875 f: -318.3599853515625 log_probs: -14.657580375671387\n",
      "Epoch 1800/100000, Loss: 458746.96875 f: -328.010009765625 log_probs: -14.309011459350586\n",
      "Epoch 1900/100000, Loss: 465139.6875 f: -339.3699951171875 log_probs: -14.113761901855469\n",
      "Epoch 2000/100000, Loss: 463784.6875 f: -336.67999267578125 log_probs: -14.177848815917969\n",
      "Epoch 2100/100000, Loss: 468979.59375 f: -347.7200012207031 log_probs: -13.832662582397461\n",
      "Epoch 2200/100000, Loss: 467498.3125 f: -354.1300048828125 log_probs: -13.636992454528809\n",
      "Epoch 2300/100000, Loss: 475931.28125 f: -375.9200134277344 log_probs: -13.0066499710083\n",
      "Epoch 2400/100000, Loss: 475738.375 f: -377.510009765625 log_probs: -13.050396919250488\n",
      "Epoch 2500/100000, Loss: 475165.1875 f: -374.0 log_probs: -13.044054985046387\n",
      "Epoch 2600/100000, Loss: 474144.5625 f: -370.6499938964844 log_probs: -13.210302352905273\n",
      "Epoch 2700/100000, Loss: 474720.53125 f: -376.760009765625 log_probs: -12.88549518585205\n",
      "Epoch 2800/100000, Loss: 463217.09375 f: -380.9800109863281 log_probs: -12.632950782775879\n",
      "Epoch 2900/100000, Loss: 461781.78125 f: -388.2099914550781 log_probs: -12.438348770141602\n",
      "Epoch 3000/100000, Loss: 468194.625 f: -405.8699951171875 log_probs: -11.950092315673828\n",
      "Epoch 3100/100000, Loss: 465927.71875 f: -402.5799865722656 log_probs: -12.048511505126953\n",
      "Epoch 3200/100000, Loss: 460791.65625 f: -408.6000061035156 log_probs: -11.7767915725708\n",
      "Epoch 3300/100000, Loss: 460020.6875 f: -412.8999938964844 log_probs: -11.502281188964844\n",
      "Epoch 3400/100000, Loss: 456738.1875 f: -424.3900146484375 log_probs: -11.14698600769043\n",
      "Epoch 3500/100000, Loss: 460368.03125 f: -419.8599853515625 log_probs: -11.41444206237793\n",
      "Epoch 3600/100000, Loss: 434930.8125 f: -435.8500061035156 log_probs: -10.367870330810547\n",
      "Epoch 3700/100000, Loss: 451902.9375 f: -429.6000061035156 log_probs: -10.854905128479004\n",
      "Epoch 3800/100000, Loss: 443073.78125 f: -430.79998779296875 log_probs: -10.743553161621094\n",
      "Epoch 3900/100000, Loss: 448450.0625 f: -429.1199951171875 log_probs: -10.873808860778809\n",
      "Epoch 4000/100000, Loss: 426506.5625 f: -448.9200134277344 log_probs: -9.823861122131348\n",
      "Epoch 4100/100000, Loss: 430418.34375 f: -444.44000244140625 log_probs: -10.13389778137207\n",
      "Epoch 4200/100000, Loss: 428495.84375 f: -446.57000732421875 log_probs: -10.00623893737793\n",
      "Epoch 4300/100000, Loss: 416607.125 f: -456.6199951171875 log_probs: -9.571797370910645\n",
      "Epoch 4400/100000, Loss: 399709.5 f: -470.8500061035156 log_probs: -8.848843574523926\n",
      "Epoch 4500/100000, Loss: 407749.4375 f: -464.5899963378906 log_probs: -9.136774063110352\n",
      "Epoch 4600/100000, Loss: 388375.5 f: -475.0799865722656 log_probs: -8.477386474609375\n",
      "Epoch 4700/100000, Loss: 394773.46875 f: -466.5400085449219 log_probs: -8.911164283752441\n",
      "Epoch 4800/100000, Loss: 376604.0625 f: -474.1600036621094 log_probs: -8.407519340515137\n",
      "Epoch 4900/100000, Loss: 390397.625 f: -478.05999755859375 log_probs: -8.4909086227417\n",
      "Epoch 5000/100000, Loss: 383286.625 f: -478.6199951171875 log_probs: -8.368767738342285\n",
      "Epoch 5100/100000, Loss: 390701.8125 f: -475.7900085449219 log_probs: -8.48434066772461\n",
      "Epoch 5200/100000, Loss: 372488.78125 f: -478.0899963378906 log_probs: -8.150979042053223\n",
      "Epoch 5300/100000, Loss: 372771.4375 f: -479.9800109863281 log_probs: -8.147643089294434\n",
      "Epoch 5400/100000, Loss: 363705.3125 f: -483.9200134277344 log_probs: -7.839106559753418\n",
      "Epoch 5500/100000, Loss: 339517.78125 f: -496.07000732421875 log_probs: -7.162319183349609\n",
      "Epoch 5600/100000, Loss: 333533.9375 f: -500.4700012207031 log_probs: -6.958502292633057\n",
      "Epoch 5700/100000, Loss: 361626.03125 f: -484.6700134277344 log_probs: -7.820648193359375\n",
      "Epoch 5800/100000, Loss: 333458.6875 f: -494.9599914550781 log_probs: -7.145937919616699\n",
      "Epoch 5900/100000, Loss: 317442.75 f: -499.8999938964844 log_probs: -6.718093395233154\n",
      "Epoch 6000/100000, Loss: 329842.59375 f: -499.30999755859375 log_probs: -6.9068603515625\n",
      "Epoch 6100/100000, Loss: 312801.53125 f: -503.9700012207031 log_probs: -6.5342254638671875\n",
      "Epoch 6200/100000, Loss: 298275.125 f: -512.1400146484375 log_probs: -6.107980728149414\n",
      "Epoch 6300/100000, Loss: 335601.84375 f: -499.55999755859375 log_probs: -6.993828296661377\n",
      "Epoch 6400/100000, Loss: 291894.4375 f: -513.4600219726562 log_probs: -5.898018836975098\n",
      "Epoch 6500/100000, Loss: 270796.0625 f: -520.4199829101562 log_probs: -5.510614395141602\n",
      "Epoch 6600/100000, Loss: 298021.90625 f: -508.0899963378906 log_probs: -6.161848068237305\n",
      "Epoch 6700/100000, Loss: 280190.9375 f: -514.219970703125 log_probs: -5.703638076782227\n",
      "Epoch 6800/100000, Loss: 296090.84375 f: -513.0 log_probs: -6.0237717628479\n",
      "Epoch 6900/100000, Loss: 257003.46875 f: -520.6199951171875 log_probs: -5.228969573974609\n",
      "Epoch 7000/100000, Loss: 295450.4375 f: -509.0299987792969 log_probs: -6.117414474487305\n",
      "Epoch 7100/100000, Loss: 266431.0 f: -516.3800048828125 log_probs: -5.488124847412109\n",
      "Epoch 7200/100000, Loss: 265774.90625 f: -518.239990234375 log_probs: -5.413541316986084\n",
      "Epoch 7300/100000, Loss: 260754.34375 f: -521.3599853515625 log_probs: -5.22116231918335\n",
      "Epoch 7400/100000, Loss: 256790.6875 f: -524.8900146484375 log_probs: -5.085631370544434\n",
      "Epoch 7500/100000, Loss: 252441.453125 f: -525.1900024414062 log_probs: -5.089649200439453\n",
      "Epoch 7600/100000, Loss: 244166.8125 f: -525.8599853515625 log_probs: -4.884707450866699\n",
      "Epoch 7700/100000, Loss: 224726.265625 f: -530.2000122070312 log_probs: -4.448446750640869\n",
      "Epoch 7800/100000, Loss: 207584.8125 f: -534.7000122070312 log_probs: -4.039742946624756\n",
      "Epoch 7900/100000, Loss: 231254.796875 f: -529.1500244140625 log_probs: -4.583129405975342\n",
      "Epoch 8000/100000, Loss: 218942.921875 f: -531.8900146484375 log_probs: -4.291378021240234\n",
      "Epoch 8100/100000, Loss: 230355.84375 f: -526.7000122070312 log_probs: -4.603273868560791\n",
      "Epoch 8200/100000, Loss: 222463.671875 f: -530.22998046875 log_probs: -4.40410041809082\n",
      "Epoch 8300/100000, Loss: 215840.1875 f: -531.8699951171875 log_probs: -4.31273889541626\n",
      "Epoch 8400/100000, Loss: 215207.6875 f: -534.6799926757812 log_probs: -4.176843643188477\n",
      "Epoch 8500/100000, Loss: 209615.25 f: -534.4199829101562 log_probs: -4.139158248901367\n",
      "Epoch 8600/100000, Loss: 204409.46875 f: -536.7000122070312 log_probs: -3.9863131046295166\n",
      "Epoch 8700/100000, Loss: 200530.671875 f: -536.1199951171875 log_probs: -3.9163849353790283\n",
      "Epoch 8800/100000, Loss: 203448.40625 f: -534.5999755859375 log_probs: -4.045741081237793\n",
      "Epoch 8900/100000, Loss: 211371.890625 f: -535.4299926757812 log_probs: -4.13830041885376\n",
      "Epoch 9000/100000, Loss: 185122.5 f: -540.4000244140625 log_probs: -3.6269032955169678\n",
      "Epoch 9100/100000, Loss: 214312.953125 f: -531.47998046875 log_probs: -4.240966796875\n",
      "Epoch 9200/100000, Loss: 204173.453125 f: -537.3900146484375 log_probs: -3.9835920333862305\n",
      "Epoch 9300/100000, Loss: 186157.359375 f: -537.3400268554688 log_probs: -3.679779291152954\n",
      "Epoch 9400/100000, Loss: 186516.046875 f: -540.3300170898438 log_probs: -3.6713976860046387\n",
      "Epoch 9500/100000, Loss: 185597.53125 f: -539.8099975585938 log_probs: -3.6178808212280273\n",
      "Epoch 9600/100000, Loss: 200204.953125 f: -537.4500122070312 log_probs: -3.887986421585083\n",
      "Epoch 9700/100000, Loss: 187849.703125 f: -537.25 log_probs: -3.668468952178955\n",
      "Epoch 9800/100000, Loss: 163389.8125 f: -546.0900268554688 log_probs: -3.1172046661376953\n",
      "Epoch 9900/100000, Loss: 170822.21875 f: -541.4199829101562 log_probs: -3.3610124588012695\n",
      "Epoch 10000/100000, Loss: 164185.328125 f: -545.989990234375 log_probs: -3.14284086227417\n",
      "Epoch 10100/100000, Loss: 165257.84375 f: -543.25 log_probs: -3.214015245437622\n",
      "Epoch 10200/100000, Loss: 170982.515625 f: -542.8599853515625 log_probs: -3.331662178039551\n",
      "Epoch 10300/100000, Loss: 150027.265625 f: -548.25 log_probs: -2.8704724311828613\n",
      "Epoch 10400/100000, Loss: 143947.15625 f: -548.0700073242188 log_probs: -2.7656402587890625\n",
      "Epoch 10500/100000, Loss: 153328.0625 f: -548.1300048828125 log_probs: -2.9373674392700195\n",
      "Epoch 10600/100000, Loss: 163634.8125 f: -544.8400268554688 log_probs: -3.1582465171813965\n",
      "Epoch 10700/100000, Loss: 154991.46875 f: -545.75 log_probs: -2.983384609222412\n",
      "Epoch 10800/100000, Loss: 160028.203125 f: -542.739990234375 log_probs: -3.154423713684082\n",
      "Epoch 10900/100000, Loss: 146465.703125 f: -547.6400146484375 log_probs: -2.854487419128418\n",
      "Epoch 11000/100000, Loss: 109656.296875 f: -553.5900268554688 log_probs: -2.084592342376709\n",
      "Epoch 11100/100000, Loss: 149972.09375 f: -548.02001953125 log_probs: -2.884854316711426\n",
      "Epoch 11200/100000, Loss: 112213.421875 f: -553.2100219726562 log_probs: -2.1163294315338135\n",
      "Epoch 11300/100000, Loss: 169788.890625 f: -543.25 log_probs: -3.3062539100646973\n",
      "Epoch 11400/100000, Loss: 145944.65625 f: -549.030029296875 log_probs: -2.776066780090332\n",
      "Epoch 11500/100000, Loss: 137058.53125 f: -549.3800048828125 log_probs: -2.622267723083496\n",
      "Epoch 11600/100000, Loss: 139429.609375 f: -550.3499755859375 log_probs: -2.6393795013427734\n",
      "Epoch 11700/100000, Loss: 135580.484375 f: -549.1699829101562 log_probs: -2.6099228858947754\n",
      "Epoch 11800/100000, Loss: 120128.640625 f: -551.5599975585938 log_probs: -2.3169078826904297\n",
      "Epoch 11900/100000, Loss: 105004.90625 f: -555.0999755859375 log_probs: -1.9845448732376099\n",
      "Epoch 12000/100000, Loss: 127838.90625 f: -549.739990234375 log_probs: -2.476224899291992\n",
      "Epoch 12100/100000, Loss: 128508.3046875 f: -551.0 log_probs: -2.47982120513916\n",
      "Epoch 12200/100000, Loss: 133955.03125 f: -547.1599731445312 log_probs: -2.631026268005371\n",
      "Epoch 12300/100000, Loss: 111485.1328125 f: -552.8300170898438 log_probs: -2.132099151611328\n",
      "Epoch 12400/100000, Loss: 141967.84375 f: -547.9500122070312 log_probs: -2.7385005950927734\n",
      "Epoch 12500/100000, Loss: 106529.9296875 f: -554.1900024414062 log_probs: -2.0158610343933105\n",
      "Epoch 12600/100000, Loss: 109838.265625 f: -552.0599975585938 log_probs: -2.154714345932007\n",
      "Epoch 12700/100000, Loss: 126394.65625 f: -553.3900146484375 log_probs: -2.383284091949463\n",
      "Epoch 12800/100000, Loss: 108352.375 f: -554.3499755859375 log_probs: -2.0394623279571533\n",
      "Epoch 12900/100000, Loss: 114792.5 f: -551.6199951171875 log_probs: -2.2160744667053223\n",
      "Epoch 13000/100000, Loss: 135511.15625 f: -548.8900146484375 log_probs: -2.632082939147949\n",
      "Epoch 13100/100000, Loss: 104886.9140625 f: -554.22998046875 log_probs: -1.998289942741394\n",
      "Epoch 13200/100000, Loss: 137583.421875 f: -549.5800170898438 log_probs: -2.6403937339782715\n",
      "Epoch 13300/100000, Loss: 101649.2109375 f: -554.510009765625 log_probs: -1.9475882053375244\n",
      "Epoch 13400/100000, Loss: 99757.4296875 f: -554.47998046875 log_probs: -1.9077950716018677\n",
      "Epoch 13500/100000, Loss: 116751.1953125 f: -552.6099853515625 log_probs: -2.200911045074463\n",
      "Epoch 13600/100000, Loss: 102809.8125 f: -553.5499877929688 log_probs: -1.970430850982666\n",
      "Epoch 13700/100000, Loss: 104344.34375 f: -555.9000244140625 log_probs: -1.9541109800338745\n",
      "Epoch 13800/100000, Loss: 94915.828125 f: -555.2100219726562 log_probs: -1.8175228834152222\n",
      "Epoch 13900/100000, Loss: 89858.984375 f: -555.3499755859375 log_probs: -1.7090743780136108\n",
      "Epoch 14000/100000, Loss: 73562.921875 f: -559.530029296875 log_probs: -1.3685604333877563\n",
      "Epoch 14100/100000, Loss: 92299.734375 f: -555.2999877929688 log_probs: -1.7728424072265625\n",
      "Epoch 14200/100000, Loss: 80334.9375 f: -558.6500244140625 log_probs: -1.515209674835205\n",
      "Epoch 14300/100000, Loss: 89682.484375 f: -556.739990234375 log_probs: -1.6952918767929077\n",
      "Epoch 14400/100000, Loss: 89681.609375 f: -556.8599853515625 log_probs: -1.7063592672348022\n",
      "Epoch 14500/100000, Loss: 107568.296875 f: -553.0 log_probs: -2.0710582733154297\n",
      "Epoch 14600/100000, Loss: 96134.5625 f: -555.8699951171875 log_probs: -1.8163812160491943\n",
      "Epoch 14700/100000, Loss: 94840.0078125 f: -555.760009765625 log_probs: -1.7984408140182495\n",
      "Epoch 14800/100000, Loss: 66572.4921875 f: -561.1300048828125 log_probs: -1.2340654134750366\n",
      "Epoch 14900/100000, Loss: 120140.7109375 f: -551.3800048828125 log_probs: -2.3345556259155273\n",
      "Epoch 15000/100000, Loss: 66379.34375 f: -560.4400024414062 log_probs: -1.2594505548477173\n",
      "Epoch 15100/100000, Loss: 93992.4453125 f: -557.510009765625 log_probs: -1.7719556093215942\n",
      "Epoch 15200/100000, Loss: 82017.4921875 f: -558.4299926757812 log_probs: -1.5381144285202026\n",
      "Epoch 15300/100000, Loss: 84786.3984375 f: -557.6199951171875 log_probs: -1.6002297401428223\n",
      "Epoch 15400/100000, Loss: 87027.90625 f: -557.2100219726562 log_probs: -1.6524444818496704\n",
      "Epoch 15500/100000, Loss: 84172.4609375 f: -557.260009765625 log_probs: -1.605481743812561\n",
      "Epoch 15600/100000, Loss: 81243.109375 f: -556.3499755859375 log_probs: -1.5651437044143677\n",
      "Epoch 15700/100000, Loss: 81409.4140625 f: -556.469970703125 log_probs: -1.5714654922485352\n",
      "Epoch 15800/100000, Loss: 86875.7109375 f: -557.9600219726562 log_probs: -1.6242382526397705\n",
      "Epoch 15900/100000, Loss: 64148.19921875 f: -560.6799926757812 log_probs: -1.1991631984710693\n",
      "Epoch 16000/100000, Loss: 51503.65625 f: -561.8699951171875 log_probs: -0.9644950032234192\n",
      "Epoch 16100/100000, Loss: 62898.41796875 f: -560.030029296875 log_probs: -1.1865180730819702\n",
      "Epoch 16200/100000, Loss: 72464.8984375 f: -559.02001953125 log_probs: -1.3730374574661255\n",
      "Epoch 16300/100000, Loss: 53879.14453125 f: -561.97998046875 log_probs: -1.0014702081680298\n",
      "Epoch 16400/100000, Loss: 86092.1484375 f: -557.969970703125 log_probs: -1.6181232929229736\n",
      "Epoch 16500/100000, Loss: 60069.1875 f: -561.5900268554688 log_probs: -1.112535834312439\n",
      "Epoch 16600/100000, Loss: 58934.37109375 f: -561.489990234375 log_probs: -1.0986601114273071\n",
      "Epoch 16700/100000, Loss: 53739.7578125 f: -562.2000122070312 log_probs: -0.9943907260894775\n",
      "Epoch 16800/100000, Loss: 77177.0859375 f: -559.4199829101562 log_probs: -1.4402647018432617\n",
      "Epoch 16900/100000, Loss: 82184.625 f: -558.469970703125 log_probs: -1.554664134979248\n",
      "Epoch 17000/100000, Loss: 68420.03125 f: -559.8499755859375 log_probs: -1.27915620803833\n",
      "Epoch 17100/100000, Loss: 55926.49609375 f: -561.6099853515625 log_probs: -1.0416961908340454\n",
      "Epoch 17200/100000, Loss: 43575.0390625 f: -563.8900146484375 log_probs: -0.795620858669281\n",
      "Epoch 17300/100000, Loss: 61818.73828125 f: -560.739990234375 log_probs: -1.1571784019470215\n",
      "Epoch 17400/100000, Loss: 65802.2890625 f: -559.969970703125 log_probs: -1.2551313638687134\n",
      "Epoch 17500/100000, Loss: 57223.515625 f: -560.2000122070312 log_probs: -1.0878864526748657\n",
      "Epoch 17600/100000, Loss: 52876.91796875 f: -562.5599975585938 log_probs: -0.9751099348068237\n",
      "Epoch 17700/100000, Loss: 86003.5625 f: -559.02001953125 log_probs: -1.6035593748092651\n",
      "Epoch 17800/100000, Loss: 53468.7265625 f: -562.3300170898438 log_probs: -0.9874341487884521\n",
      "Epoch 17900/100000, Loss: 62985.77734375 f: -559.6599731445312 log_probs: -1.1923766136169434\n",
      "Epoch 18000/100000, Loss: 71617.890625 f: -559.5900268554688 log_probs: -1.3471176624298096\n",
      "Epoch 18100/100000, Loss: 42219.171875 f: -563.27001953125 log_probs: -0.783395528793335\n",
      "Epoch 18200/100000, Loss: 48889.2109375 f: -562.8300170898438 log_probs: -0.9097812175750732\n",
      "Epoch 18300/100000, Loss: 70693.71875 f: -557.6300048828125 log_probs: -1.400607943534851\n",
      "Epoch 18400/100000, Loss: 65342.1171875 f: -559.3499755859375 log_probs: -1.257258415222168\n",
      "Epoch 18500/100000, Loss: 56520.16015625 f: -560.5599975585938 log_probs: -1.0711753368377686\n",
      "Epoch 18600/100000, Loss: 41982.15234375 f: -562.989990234375 log_probs: -0.8002657890319824\n",
      "Epoch 18700/100000, Loss: 60520.91796875 f: -560.969970703125 log_probs: -1.1362338066101074\n",
      "Epoch 18800/100000, Loss: 55538.1015625 f: -561.0800170898438 log_probs: -1.0492637157440186\n",
      "Epoch 18900/100000, Loss: 59249.4765625 f: -560.75 log_probs: -1.1119364500045776\n",
      "Epoch 19000/100000, Loss: 67969.640625 f: -558.6400146484375 log_probs: -1.2967718839645386\n",
      "Epoch 19100/100000, Loss: 64936.59765625 f: -558.6199951171875 log_probs: -1.2420754432678223\n",
      "Epoch 19200/100000, Loss: 45184.23046875 f: -561.5599975585938 log_probs: -0.8889154195785522\n",
      "Epoch 19300/100000, Loss: 68392.7109375 f: -560.1599731445312 log_probs: -1.286927342414856\n",
      "Epoch 19400/100000, Loss: 32908.58984375 f: -564.6099853515625 log_probs: -0.6031726002693176\n",
      "Epoch 19500/100000, Loss: 69067.765625 f: -559.4600219726562 log_probs: -1.3130890130996704\n",
      "Epoch 19600/100000, Loss: 39983.234375 f: -563.260009765625 log_probs: -0.7549620270729065\n",
      "Epoch 19700/100000, Loss: 44008.65625 f: -562.010009765625 log_probs: -0.8433258533477783\n",
      "Epoch 19800/100000, Loss: 31413.2109375 f: -565.1699829101562 log_probs: -0.570092499256134\n",
      "Epoch 19900/100000, Loss: 44528.54296875 f: -562.010009765625 log_probs: -0.8408093452453613\n",
      "Epoch 20000/100000, Loss: 38126.9453125 f: -563.8599853515625 log_probs: -0.7075613141059875\n",
      "Epoch 20100/100000, Loss: 53518.94140625 f: -560.989990234375 log_probs: -1.0199096202850342\n",
      "Epoch 20200/100000, Loss: 50372.36328125 f: -561.7100219726562 log_probs: -0.9576922655105591\n",
      "Epoch 20300/100000, Loss: 44104.99609375 f: -563.1099853515625 log_probs: -0.8184828758239746\n",
      "Epoch 20400/100000, Loss: 43139.5234375 f: -563.6900024414062 log_probs: -0.791689395904541\n",
      "Epoch 20500/100000, Loss: 37445.3203125 f: -563.5599975585938 log_probs: -0.6981081366539001\n",
      "Epoch 20600/100000, Loss: 43463.3203125 f: -563.0900268554688 log_probs: -0.8085349798202515\n",
      "Epoch 20700/100000, Loss: 49249.15625 f: -562.5399780273438 log_probs: -0.9233673214912415\n",
      "Epoch 20800/100000, Loss: 41113.140625 f: -563.4099731445312 log_probs: -0.774064838886261\n",
      "Epoch 20900/100000, Loss: 51094.6484375 f: -561.6400146484375 log_probs: -0.9627101421356201\n",
      "Epoch 21000/100000, Loss: 44873.5 f: -562.6900024414062 log_probs: -0.8450549840927124\n",
      "Epoch 21100/100000, Loss: 46640.86328125 f: -562.6300048828125 log_probs: -0.8728051781654358\n",
      "Epoch 21200/100000, Loss: 48894.76953125 f: -562.260009765625 log_probs: -0.921884298324585\n",
      "Epoch 21300/100000, Loss: 41058.2109375 f: -563.75 log_probs: -0.7585504055023193\n",
      "Epoch 21400/100000, Loss: 42853.87109375 f: -562.47998046875 log_probs: -0.8185167908668518\n",
      "Epoch 21500/100000, Loss: 48263.86328125 f: -562.0800170898438 log_probs: -0.9080917239189148\n",
      "Epoch 21600/100000, Loss: 51182.61328125 f: -561.6199951171875 log_probs: -0.9640421867370605\n",
      "Epoch 21700/100000, Loss: 34032.36328125 f: -563.1699829101562 log_probs: -0.6495500802993774\n",
      "Epoch 21800/100000, Loss: 54170.6875 f: -562.4000244140625 log_probs: -1.0181396007537842\n",
      "Epoch 21900/100000, Loss: 49377.25 f: -562.739990234375 log_probs: -0.915465235710144\n",
      "Epoch 22000/100000, Loss: 31201.376953125 f: -564.47998046875 log_probs: -0.5757327079772949\n",
      "Epoch 22100/100000, Loss: 30058.962890625 f: -564.1199951171875 log_probs: -0.5814739465713501\n",
      "Epoch 22200/100000, Loss: 31475.30859375 f: -563.6300048828125 log_probs: -0.595696210861206\n",
      "Epoch 22300/100000, Loss: 30083.41796875 f: -564.260009765625 log_probs: -0.5678155422210693\n",
      "Epoch 22400/100000, Loss: 36663.171875 f: -563.1400146484375 log_probs: -0.6919384598731995\n",
      "Epoch 22500/100000, Loss: 41165.77734375 f: -563.469970703125 log_probs: -0.7637389898300171\n",
      "Epoch 22600/100000, Loss: 24919.3984375 f: -564.5900268554688 log_probs: -0.46877720952033997\n",
      "Epoch 22700/100000, Loss: 52411.9453125 f: -561.77001953125 log_probs: -0.9846994280815125\n",
      "Epoch 22800/100000, Loss: 40495.3125 f: -563.2000122070312 log_probs: -0.7582790851593018\n",
      "Epoch 22900/100000, Loss: 44275.2109375 f: -562.530029296875 log_probs: -0.8456831574440002\n",
      "Epoch 23000/100000, Loss: 47415.89453125 f: -562.0900268554688 log_probs: -0.892387866973877\n",
      "Epoch 23100/100000, Loss: 29779.955078125 f: -564.8200073242188 log_probs: -0.549005925655365\n",
      "Epoch 23200/100000, Loss: 19385.34765625 f: -565.260009765625 log_probs: -0.3626154959201813\n",
      "Epoch 23300/100000, Loss: 37797.125 f: -563.3800048828125 log_probs: -0.7103651165962219\n",
      "Epoch 23400/100000, Loss: 20706.900390625 f: -565.8499755859375 log_probs: -0.37707287073135376\n",
      "Epoch 23500/100000, Loss: 32707.732421875 f: -563.9299926757812 log_probs: -0.620956540107727\n",
      "Epoch 23600/100000, Loss: 32583.91796875 f: -564.469970703125 log_probs: -0.6024863719940186\n",
      "Epoch 23700/100000, Loss: 21830.09375 f: -565.0900268554688 log_probs: -0.40845397114753723\n",
      "Epoch 23800/100000, Loss: 31683.603515625 f: -563.760009765625 log_probs: -0.5971546769142151\n",
      "Epoch 23900/100000, Loss: 37569.83203125 f: -563.6400146484375 log_probs: -0.7169079780578613\n",
      "Epoch 24000/100000, Loss: 37650.828125 f: -562.97998046875 log_probs: -0.7147810459136963\n",
      "Epoch 24100/100000, Loss: 40817.42578125 f: -563.0599975585938 log_probs: -0.7669792175292969\n",
      "Epoch 24200/100000, Loss: 28702.173828125 f: -565.1900024414062 log_probs: -0.5229823589324951\n",
      "Epoch 24300/100000, Loss: 36956.73046875 f: -564.2000122070312 log_probs: -0.6822507977485657\n",
      "Epoch 24400/100000, Loss: 31918.017578125 f: -562.8300170898438 log_probs: -0.6178929805755615\n",
      "Epoch 24500/100000, Loss: 36007.78515625 f: -563.22998046875 log_probs: -0.6797364950180054\n",
      "Epoch 24600/100000, Loss: 19614.935546875 f: -565.7000122070312 log_probs: -0.3636110723018646\n",
      "Epoch 24700/100000, Loss: 31794.06640625 f: -564.5399780273438 log_probs: -0.5911911725997925\n",
      "Epoch 24800/100000, Loss: 31651.66015625 f: -563.3699951171875 log_probs: -0.6359942555427551\n",
      "Epoch 24900/100000, Loss: 29676.54296875 f: -564.27001953125 log_probs: -0.5552886724472046\n",
      "Epoch 25000/100000, Loss: 35850.7578125 f: -563.010009765625 log_probs: -0.6831890940666199\n",
      "Epoch 25100/100000, Loss: 45260.01171875 f: -562.97998046875 log_probs: -0.8541917204856873\n",
      "Epoch 25200/100000, Loss: 18705.12109375 f: -565.2899780273438 log_probs: -0.3527393043041229\n",
      "Epoch 25300/100000, Loss: 27356.06640625 f: -564.7100219726562 log_probs: -0.5115781426429749\n",
      "Epoch 25400/100000, Loss: 10380.4140625 f: -566.6599731445312 log_probs: -0.18635815382003784\n",
      "Epoch 25500/100000, Loss: 25691.099609375 f: -564.6599731445312 log_probs: -0.4818956255912781\n",
      "Epoch 25600/100000, Loss: 28398.435546875 f: -564.3400268554688 log_probs: -0.5341465473175049\n",
      "Epoch 25700/100000, Loss: 22207.490234375 f: -565.5499877929688 log_probs: -0.4068434238433838\n",
      "Epoch 25800/100000, Loss: 17237.4921875 f: -565.2899780273438 log_probs: -0.3257410526275635\n",
      "Epoch 25900/100000, Loss: 15630.4658203125 f: -565.6900024414062 log_probs: -0.29095780849456787\n",
      "Epoch 26000/100000, Loss: 13026.087890625 f: -566.2100219726562 log_probs: -0.23812510073184967\n",
      "Epoch 26100/100000, Loss: 32048.61328125 f: -563.2000122070312 log_probs: -0.6145551800727844\n",
      "Epoch 26200/100000, Loss: 21255.046875 f: -565.1099853515625 log_probs: -0.39860329031944275\n",
      "Epoch 26300/100000, Loss: 7702.162109375 f: -566.5499877929688 log_probs: -0.1413145810365677\n",
      "Epoch 26400/100000, Loss: 13770.7421875 f: -566.1900024414062 log_probs: -0.2515541911125183\n",
      "Epoch 26500/100000, Loss: 30918.998046875 f: -563.9199829101562 log_probs: -0.5823014974594116\n",
      "Epoch 26600/100000, Loss: 23440.927734375 f: -564.75 log_probs: -0.44164538383483887\n",
      "Epoch 26700/100000, Loss: 23520.47265625 f: -564.9199829101562 log_probs: -0.43989112973213196\n",
      "Epoch 26800/100000, Loss: 10918.427734375 f: -566.030029296875 log_probs: -0.2059628665447235\n",
      "Epoch 26900/100000, Loss: 24913.79296875 f: -565.27001953125 log_probs: -0.4580501616001129\n",
      "Epoch 27000/100000, Loss: 39952.77734375 f: -563.0 log_probs: -0.754346489906311\n",
      "Epoch 27100/100000, Loss: 42795.8046875 f: -563.1199951171875 log_probs: -0.8005847334861755\n",
      "Epoch 27200/100000, Loss: 27933.017578125 f: -563.2100219726562 log_probs: -0.5432553887367249\n",
      "Epoch 27300/100000, Loss: 24510.35546875 f: -564.5599975585938 log_probs: -0.4640868902206421\n",
      "Epoch 27400/100000, Loss: 20534.484375 f: -565.5900268554688 log_probs: -0.3788239359855652\n",
      "Epoch 27500/100000, Loss: 24253.767578125 f: -564.010009765625 log_probs: -0.4672897756099701\n",
      "Epoch 27600/100000, Loss: 26499.73046875 f: -563.1900024414062 log_probs: -0.5219496488571167\n",
      "Epoch 27700/100000, Loss: 12495.4296875 f: -566.3400268554688 log_probs: -0.22693035006523132\n",
      "Epoch 27800/100000, Loss: 39457.25 f: -563.8800048828125 log_probs: -0.7300349473953247\n",
      "Epoch 27900/100000, Loss: 16841.2265625 f: -565.9299926757812 log_probs: -0.30960530042648315\n",
      "Epoch 28000/100000, Loss: 45150.01171875 f: -562.7100219726562 log_probs: -0.8477638363838196\n",
      "Epoch 28100/100000, Loss: 20613.98828125 f: -565.3099975585938 log_probs: -0.3835807740688324\n",
      "Epoch 28200/100000, Loss: 25385.787109375 f: -564.8200073242188 log_probs: -0.47409844398498535\n",
      "Epoch 28300/100000, Loss: 19794.763671875 f: -565.219970703125 log_probs: -0.371248334646225\n",
      "Epoch 28400/100000, Loss: 31043.1015625 f: -564.3400268554688 log_probs: -0.5810115933418274\n",
      "Epoch 28500/100000, Loss: 41525.2734375 f: -563.280029296875 log_probs: -0.7768219113349915\n",
      "Epoch 28600/100000, Loss: 16658.259765625 f: -565.6500244140625 log_probs: -0.3100636601448059\n",
      "Epoch 28700/100000, Loss: 37632.35546875 f: -564.2899780273438 log_probs: -0.6931730508804321\n",
      "Epoch 28800/100000, Loss: 16180.51171875 f: -566.02001953125 log_probs: -0.29585209488868713\n",
      "Epoch 28900/100000, Loss: 24881.748046875 f: -564.010009765625 log_probs: -0.47976115345954895\n",
      "Epoch 29000/100000, Loss: 30147.6875 f: -563.280029296875 log_probs: -0.604691743850708\n",
      "Epoch 29100/100000, Loss: 5829.83056640625 f: -566.8300170898438 log_probs: -0.10423558950424194\n",
      "Epoch 29200/100000, Loss: 13623.357421875 f: -565.8599853515625 log_probs: -0.25453853607177734\n",
      "Epoch 29300/100000, Loss: 34559.77734375 f: -563.7100219726562 log_probs: -0.6511829495429993\n",
      "Epoch 29400/100000, Loss: 17542.19921875 f: -565.3499755859375 log_probs: -0.32974758744239807\n",
      "Epoch 29500/100000, Loss: 16867.765625 f: -565.4000244140625 log_probs: -0.31885281205177307\n",
      "Epoch 29600/100000, Loss: 16987.423828125 f: -565.3300170898438 log_probs: -0.321432501077652\n",
      "Epoch 29700/100000, Loss: 11884.1689453125 f: -566.489990234375 log_probs: -0.21441158652305603\n",
      "Epoch 29800/100000, Loss: 20417.6171875 f: -564.8499755859375 log_probs: -0.38824984431266785\n",
      "Epoch 29900/100000, Loss: 10512.8408203125 f: -566.1199951171875 log_probs: -0.1967509686946869\n",
      "Epoch 30000/100000, Loss: 17205.8125 f: -565.0900268554688 log_probs: -0.32912564277648926\n",
      "Epoch 30100/100000, Loss: 18466.984375 f: -565.52001953125 log_probs: -0.34385931491851807\n",
      "Epoch 30200/100000, Loss: 17563.16796875 f: -565.4600219726562 log_probs: -0.32878777384757996\n",
      "Epoch 30300/100000, Loss: 9936.5537109375 f: -566.3800048828125 log_probs: -0.1828695684671402\n",
      "Epoch 30400/100000, Loss: 9874.6328125 f: -566.510009765625 log_probs: -0.17979678511619568\n",
      "Epoch 30500/100000, Loss: 20354.923828125 f: -565.4199829101562 log_probs: -0.37769314646720886\n",
      "Epoch 30600/100000, Loss: 12440.400390625 f: -566.2100219726562 log_probs: -0.2283373326063156\n",
      "Epoch 30700/100000, Loss: 28384.728515625 f: -565.25 log_probs: -0.5234329104423523\n",
      "Epoch 30800/100000, Loss: 19109.818359375 f: -565.4000244140625 log_probs: -0.35694748163223267\n",
      "Epoch 30900/100000, Loss: 12958.89453125 f: -566.2100219726562 log_probs: -0.23779267072677612\n",
      "Epoch 31000/100000, Loss: 17762.8359375 f: -565.02001953125 log_probs: -0.34077730774879456\n",
      "Epoch 31100/100000, Loss: 26556.265625 f: -563.239990234375 log_probs: -0.5232437252998352\n",
      "Epoch 31200/100000, Loss: 16269.8095703125 f: -566.02001953125 log_probs: -0.29801204800605774\n",
      "Epoch 31300/100000, Loss: 9807.533203125 f: -565.989990234375 log_probs: -0.20001858472824097\n",
      "Epoch 31400/100000, Loss: 2707.991943359375 f: -567.0 log_probs: -0.047760009765625\n",
      "Epoch 31500/100000, Loss: 10376.16015625 f: -566.22998046875 log_probs: -0.19303688406944275\n",
      "Epoch 31600/100000, Loss: 15821.9443359375 f: -565.8900146484375 log_probs: -0.29184433817863464\n",
      "Epoch 31700/100000, Loss: 16462.623046875 f: -565.8900146484375 log_probs: -0.3035551905632019\n",
      "Epoch 31800/100000, Loss: 11580.5947265625 f: -566.489990234375 log_probs: -0.20914943516254425\n",
      "Epoch 31900/100000, Loss: 22932.365234375 f: -565.1900024414062 log_probs: -0.427653968334198\n",
      "Epoch 32000/100000, Loss: 14598.0673828125 f: -565.1900024414062 log_probs: -0.2837894558906555\n",
      "Epoch 32100/100000, Loss: 19593.822265625 f: -565.3900146484375 log_probs: -0.36573323607444763\n",
      "Epoch 32200/100000, Loss: 23391.99609375 f: -565.2000122070312 log_probs: -0.4358218312263489\n",
      "Epoch 32300/100000, Loss: 26666.8046875 f: -565.27001953125 log_probs: -0.4909757971763611\n",
      "Epoch 32400/100000, Loss: 17169.224609375 f: -565.6500244140625 log_probs: -0.3206447660923004\n",
      "Epoch 32500/100000, Loss: 18663.705078125 f: -565.8699951171875 log_probs: -0.3418026268482208\n",
      "Epoch 32600/100000, Loss: 15644.4013671875 f: -565.6900024414062 log_probs: -0.29213008284568787\n",
      "Epoch 32700/100000, Loss: 29679.625 f: -564.6699829101562 log_probs: -0.5516871809959412\n",
      "Epoch 32800/100000, Loss: 18712.53125 f: -565.4500122070312 log_probs: -0.35427120327949524\n",
      "Epoch 32900/100000, Loss: 22919.173828125 f: -564.5499877929688 log_probs: -0.4375845789909363\n",
      "Epoch 33000/100000, Loss: 14770.7578125 f: -566.3200073242188 log_probs: -0.26725760102272034\n",
      "Epoch 33100/100000, Loss: 9679.595703125 f: -566.3800048828125 log_probs: -0.17845281958580017\n",
      "Epoch 33200/100000, Loss: 18040.830078125 f: -566.1500244140625 log_probs: -0.3267442584037781\n",
      "Epoch 33300/100000, Loss: 15648.763671875 f: -566.1699829101562 log_probs: -0.28506791591644287\n",
      "Epoch 33400/100000, Loss: 12825.439453125 f: -566.0999755859375 log_probs: -0.2375115603208542\n",
      "Epoch 33500/100000, Loss: 13177.3515625 f: -566.010009765625 log_probs: -0.24591992795467377\n",
      "Epoch 33600/100000, Loss: 26760.521484375 f: -564.4000244140625 log_probs: -0.5058125853538513\n",
      "Epoch 33700/100000, Loss: 15805.248046875 f: -565.5599975585938 log_probs: -0.29712796211242676\n",
      "Epoch 33800/100000, Loss: 9257.306640625 f: -566.510009765625 log_probs: -0.16886228322982788\n",
      "Epoch 33900/100000, Loss: 19922.57421875 f: -565.0399780273438 log_probs: -0.37818145751953125\n",
      "Epoch 34000/100000, Loss: 12903.5400390625 f: -566.3400268554688 log_probs: -0.23533083498477936\n",
      "Epoch 34100/100000, Loss: 13295.294921875 f: -565.72998046875 log_probs: -0.25173938274383545\n",
      "Epoch 34200/100000, Loss: 13350.1318359375 f: -565.469970703125 log_probs: -0.2570939064025879\n",
      "Epoch 34300/100000, Loss: 16802.564453125 f: -565.4099731445312 log_probs: -0.31734374165534973\n",
      "Epoch 34400/100000, Loss: 15896.5439453125 f: -566.02001953125 log_probs: -0.291413277387619\n",
      "Epoch 34500/100000, Loss: 14769.880859375 f: -565.219970703125 log_probs: -0.2865501344203949\n",
      "Epoch 34600/100000, Loss: 5014.52099609375 f: -566.8300170898438 log_probs: -0.0899336040019989\n",
      "Epoch 34700/100000, Loss: 15398.0029296875 f: -565.719970703125 log_probs: -0.287689208984375\n",
      "Epoch 34800/100000, Loss: 21679.869140625 f: -564.4500122070312 log_probs: -0.4279061257839203\n",
      "Epoch 34900/100000, Loss: 19753.1953125 f: -565.52001953125 log_probs: -0.3679780960083008\n",
      "Epoch 35000/100000, Loss: 31643.107421875 f: -564.3900146484375 log_probs: -0.5957463979721069\n",
      "Epoch 35100/100000, Loss: 25326.419921875 f: -564.9099731445312 log_probs: -0.47261133790016174\n",
      "Epoch 35200/100000, Loss: 15282.1181640625 f: -566.3200073242188 log_probs: -0.27672356367111206\n",
      "Epoch 35300/100000, Loss: 23271.7109375 f: -565.25 log_probs: -0.4317346215248108\n",
      "Epoch 35400/100000, Loss: 12849.087890625 f: -566.3400268554688 log_probs: -0.23440369963645935\n",
      "Epoch 35500/100000, Loss: 7781.33984375 f: -566.6599731445312 log_probs: -0.14033019542694092\n",
      "Epoch 35600/100000, Loss: 16286.162109375 f: -565.739990234375 log_probs: -0.30269181728363037\n",
      "Epoch 35700/100000, Loss: 12568.3564453125 f: -565.5700073242188 log_probs: -0.24761049449443817\n",
      "Epoch 35800/100000, Loss: 4953.146484375 f: -566.8300170898438 log_probs: -0.0889081060886383\n",
      "Epoch 35900/100000, Loss: 12956.3330078125 f: -566.010009765625 log_probs: -0.24154725670814514\n",
      "Epoch 36000/100000, Loss: 23387.51953125 f: -565.0499877929688 log_probs: -0.4379782974720001\n",
      "Epoch 36100/100000, Loss: 17034.32421875 f: -565.4199829101562 log_probs: -0.3217832148075104\n",
      "Epoch 36200/100000, Loss: 25435.01171875 f: -563.8800048828125 log_probs: -0.4943733215332031\n",
      "Epoch 36300/100000, Loss: 25208.13671875 f: -564.9199829101562 log_probs: -0.47318512201309204\n",
      "Epoch 36400/100000, Loss: 37953.96875 f: -563.5 log_probs: -0.7161189317703247\n",
      "Epoch 36500/100000, Loss: 12499.748046875 f: -566.2100219726562 log_probs: -0.23014427721500397\n",
      "Epoch 36600/100000, Loss: 15803.248046875 f: -566.1699829101562 log_probs: -0.2881164252758026\n",
      "Epoch 36700/100000, Loss: 22979.974609375 f: -565.0700073242188 log_probs: -0.4300907552242279\n",
      "Epoch 36800/100000, Loss: 10349.1318359375 f: -565.6699829101562 log_probs: -0.20212554931640625\n",
      "Epoch 36900/100000, Loss: 25057.55078125 f: -564.0800170898438 log_probs: -0.48416435718536377\n",
      "Epoch 37000/100000, Loss: 20788.3359375 f: -565.1900024414062 log_probs: -0.39203688502311707\n",
      "Epoch 37100/100000, Loss: 26106.283203125 f: -564.1300048828125 log_probs: -0.5019460320472717\n",
      "Epoch 37200/100000, Loss: 8298.005859375 f: -566.6599731445312 log_probs: -0.1498410999774933\n",
      "Epoch 37300/100000, Loss: 12914.4287109375 f: -565.739990234375 log_probs: -0.24466221034526825\n",
      "Epoch 37400/100000, Loss: 12641.94921875 f: -565.739990234375 log_probs: -0.24018627405166626\n",
      "Epoch 37500/100000, Loss: 9194.658203125 f: -566.6599731445312 log_probs: -0.16616478562355042\n",
      "Epoch 37600/100000, Loss: 12305.6884765625 f: -566.0999755859375 log_probs: -0.22857140004634857\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[38]\u001b[39m\u001b[32m, line 10\u001b[39m\n\u001b[32m      8\u001b[39m f_values, log_probs = p.forward()\n\u001b[32m      9\u001b[39m loss = p.surrogate_loss(f_values, log_probs)\n\u001b[32m---> \u001b[39m\u001b[32m10\u001b[39m \u001b[43mloss\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     11\u001b[39m optimizer.step()\n\u001b[32m     13\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (epoch + \u001b[32m1\u001b[39m) % \u001b[32m100\u001b[39m == \u001b[32m0\u001b[39m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Projects/condensed/hubbard-transformer/.venv/lib/python3.13/site-packages/torch/_tensor.py:626\u001b[39m, in \u001b[36mTensor.backward\u001b[39m\u001b[34m(self, gradient, retain_graph, create_graph, inputs)\u001b[39m\n\u001b[32m    616\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    617\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[32m    618\u001b[39m         Tensor.backward,\n\u001b[32m    619\u001b[39m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[32m   (...)\u001b[39m\u001b[32m    624\u001b[39m         inputs=inputs,\n\u001b[32m    625\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m626\u001b[39m \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mautograd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    627\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m=\u001b[49m\u001b[43minputs\u001b[49m\n\u001b[32m    628\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Projects/condensed/hubbard-transformer/.venv/lib/python3.13/site-packages/torch/autograd/__init__.py:340\u001b[39m, in \u001b[36mbackward\u001b[39m\u001b[34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[39m\n\u001b[32m    331\u001b[39m inputs = (\n\u001b[32m    332\u001b[39m     (inputs,)\n\u001b[32m    333\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(inputs, (torch.Tensor, graph.GradientEdge))\n\u001b[32m   (...)\u001b[39m\u001b[32m    336\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m ()\n\u001b[32m    337\u001b[39m )\n\u001b[32m    339\u001b[39m grad_tensors_ = _tensor_or_tensors_to_tuple(grad_tensors, \u001b[38;5;28mlen\u001b[39m(tensors))\n\u001b[32m--> \u001b[39m\u001b[32m340\u001b[39m grad_tensors_ = \u001b[43m_make_grads\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_grads_batched\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m    341\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m retain_graph \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    342\u001b[39m     retain_graph = create_graph\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Projects/condensed/hubbard-transformer/.venv/lib/python3.13/site-packages/torch/autograd/__init__.py:95\u001b[39m, in \u001b[36m_make_grads\u001b[39m\u001b[34m(outputs, grads, is_grads_batched)\u001b[39m\n\u001b[32m     93\u001b[39m new_grads: List[_OptionalTensor] = []\n\u001b[32m     94\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m out, grad \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(outputs, grads):\n\u001b[32m---> \u001b[39m\u001b[32m95\u001b[39m     out = \u001b[43mcast\u001b[49m\u001b[43m(\u001b[49m\u001b[43mUnion\u001b[49m\u001b[43m[\u001b[49m\u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mTensor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgraph\u001b[49m\u001b[43m.\u001b[49m\u001b[43mGradientEdge\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     96\u001b[39m     out_size = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m     97\u001b[39m     out_device = \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/share/uv/python/cpython-3.13.3-macos-aarch64-none/lib/python3.13/typing.py:2371\u001b[39m, in \u001b[36mcast\u001b[39m\u001b[34m(typ, val)\u001b[39m\n\u001b[32m   2367\u001b[39m                 \u001b[38;5;28mcls\u001b[39m.__non_callable_proto_members__.add(attr)\n\u001b[32m   2368\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mcls\u001b[39m\n\u001b[32m-> \u001b[39m\u001b[32m2371\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mcast\u001b[39m(typ, val):\n\u001b[32m   2372\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Cast a value to a type.\u001b[39;00m\n\u001b[32m   2373\u001b[39m \n\u001b[32m   2374\u001b[39m \u001b[33;03m    This returns the value unchanged.  To the type checker this\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m   2377\u001b[39m \u001b[33;03m    to be as fast as possible).\u001b[39;00m\n\u001b[32m   2378\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m   2379\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m val\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# Define optimizer\n",
    "optimizer = torch.optim.Adam(p.parameters(), lr=1e-3)\n",
    "\n",
    "num_epochs = 100000\n",
    "for epoch in range(num_epochs):\n",
    "    optimizer.zero_grad()  # Clear gradients\n",
    "\n",
    "    f_values, log_probs = p.forward()\n",
    "    loss = p.surrogate_loss(f_values, log_probs)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    if (epoch + 1) % 100 == 0:\n",
    "        print(\n",
    "            f\"Epoch {epoch + 1}/{num_epochs}, Loss: {loss.item()} f: {f_values.mean().item()} log_probs: {log_probs.mean().item()}\"\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
