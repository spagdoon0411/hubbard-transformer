{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The goal of this book is to justify to myself that the implementation of multiheaded attention is right."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n",
      "CUDA is available.\n",
      "Changed working dir to /home/spandan/Projects/hubbard-transformer\n"
     ]
    }
   ],
   "source": [
    "from boilerplate import setup_nb\n",
    "import torch\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "device = setup_nb()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TOKEN_DIMS = 16\n",
    "PARAM_DIMS = 32\n",
    "EMBED_DIMS = TOKEN_DIMS + PARAM_DIMS\n",
    "N_PARAMS = 10\n",
    "MAX_LEN = 100\n",
    "WAVELEN_FACT = 1e6\n",
    "MAX_OCC = 5\n",
    "MIN_OCC = 0\n",
    "BATCH = 32\n",
    "N_HEADS = 8\n",
    "KEY_DIMS = 12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "from embedding import HubbardEmbedding\n",
    "\n",
    "init_len = 10\n",
    "\n",
    "he = HubbardEmbedding(TOKEN_DIMS, PARAM_DIMS, N_PARAMS, MAX_LEN, WAVELEN_FACT)\n",
    "he.to(device)\n",
    "params = torch.randn(N_PARAMS, BATCH).to(device)\n",
    "occupations = torch.randint(1, 5, (init_len, BATCH)).to(device)\n",
    "logits = he(params, occupations).to(device, dtype=torch.complex64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "import einops as ein"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([20, 32, 64])"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logits.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of logits before attention: torch.Size([20, 32, 64])\n",
      "Using a value space of dimension 8\n",
      "Shape of logits after attention: torch.Size([20, 32, 64])\n"
     ]
    }
   ],
   "source": [
    "from complex_model import ComplexAttention\n",
    "\n",
    "print(\"Shape of logits before attention:\", logits.shape)\n",
    "ca = ComplexAttention(EMBED_DIMS, KEY_DIMS, N_HEADS, MAX_LEN).to(device)\n",
    "logits = ca.forward(logits)\n",
    "print(\"Shape of logits after attention:\", logits.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[  8.3989+13.3636j,   0.4310-29.0144j,   5.3808+13.9602j,\n",
       "          ...,   7.4629+1.6668j,  38.0070+9.0520j,\n",
       "          19.0019-2.2060j],\n",
       "        [  9.2172+13.3212j,   0.3921-28.9608j,   5.8645+13.9536j,\n",
       "          ...,   7.4354+1.6831j,  37.9700+9.0356j,\n",
       "          18.9245-2.1504j],\n",
       "        [ 12.5984+20.0453j,   0.6466-43.5216j,   8.0712+20.9404j,\n",
       "          ...,  11.1943+2.5002j,  57.0105+13.5780j,\n",
       "          28.5028-3.3091j],\n",
       "        ...,\n",
       "        [ 30.9294-22.2259j, -25.0534+1.1900j, -12.9271+7.1966j,\n",
       "          ...,  10.1066+12.5140j,  21.1985+12.8976j,\n",
       "          27.8866-35.0836j],\n",
       "        [  6.7175+8.7586j, -17.1525+18.2251j,   3.9634+21.8257j,\n",
       "          ...,  61.8793+20.0443j, -12.9123-15.1276j,\n",
       "          34.3297+23.7823j],\n",
       "        [-14.0472-23.5252j, -10.2845+6.1496j,   1.6564+2.2612j,\n",
       "          ...,  24.9164+22.6090j,  -7.1596-17.9396j,\n",
       "          19.0006+38.1621j]], device='cuda:0', grad_fn=<SliceBackward0>)"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logits[:, 0, :]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Attention mechanism attention patterns as einsum inner products\n",
    "\n",
    "If you matmul two matrices you create a matrix of inner products\n",
    "\n",
    "$$ \n",
    "C_{ij} \n",
    "\n",
    "= A_{ik} B_{kj}\n",
    "$$\n",
    "\n",
    "The specific case for query and key matul is:\n",
    "\n",
    "$$\n",
    "\n",
    "C_{sS}\n",
    "\n",
    "= (K_{ks})^H Q_{kS}\n",
    "\n",
    "= K'_{sk} Q_{kS}\n",
    "\n",
    "= C\n",
    "\n",
    "$$\n",
    "\n",
    "Note that the axis s is the \"same\" as the axis S but they need different names for einsum. \n",
    "\n",
    "This matmul is all possible inner products over adjacency relationships and is the $\\braket{K, Q}$ described in the paper.\n",
    "\n",
    "We need to compute a $\\braket{k|q}$ for each query and key vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The softmax for the pattern is applied across the axis of summation that the linear combination from E to E' happens on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[False,  True,  True,  True,  True],\n",
       "        [False, False,  True,  True,  True],\n",
       "        [False, False, False,  True,  True],\n",
       "        [False, False, False, False,  True],\n",
       "        [False, False, False, False, False]])"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "torch.triu(torch.ones(5, 5), diagonal=1).bool()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0710,    -inf,    -inf,    -inf,    -inf],\n",
       "        [-0.7091, -0.4091,    -inf,    -inf,    -inf],\n",
       "        [ 0.4236,  0.5352,  0.5777,    -inf,    -inf],\n",
       "        [-0.0455, -0.6346, -0.2280, -0.6564,    -inf],\n",
       "        [-1.0733, -0.9297,  0.2490,  0.1138, -0.4720]])"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# e.g., the fourth embedding update (row) should only be\n",
    "# computed using the first four tokens--which is why\n",
    "# there are only four unmasked relevances in the fourth row\n",
    "torch.randn(5, 5).masked_fill(\n",
    "    torch.triu(torch.ones(5, 5), diagonal=1).bool(), torch.tensor(float(\"-inf\"))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tqs2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
