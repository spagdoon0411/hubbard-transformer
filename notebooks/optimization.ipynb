{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA is not available. Running on CPU.\n",
      "Changed working dir to /Users/spandan/Projects/condensed/hubbard-transformer\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from boilerplate import setup_nb\n",
    "\n",
    "device = setup_nb()\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import neptune\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "import torch\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load_dotenv(\".env\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[neptune] [info   ] Neptune initialized. Open in the app: https://app.neptune.ai/spagdoon0411/condensed/e/CON-22\n"
     ]
    }
   ],
   "source": [
    "run = neptune.init_run(\n",
    "    project=\"spagdoon0411/condensed\",\n",
    "    api_token=os.environ[\"NEPTUNE_API_TOKEN\"],\n",
    ") \n",
    "\n",
    "params = {\n",
    "    \"learning_rate\": 1e-3, \n",
    "    \"batch_size\": 32,\n",
    "    \"n_sites\": 5,\n",
    "    \"embed_dim\": 32,\n",
    "    \"n_heads\": 2,\n",
    "    \"n_layers\": 2, \n",
    "    \"dim_feedforward\": 64,\n",
    "    \"particle_number\": 3,\n",
    "    \"max_len\": 100,\n",
    "    \"t\": 1.0,\n",
    "    \"U\": 2.0,\n",
    "    \"epochs\": 10000,\n",
    "    \"device\": str(device),\n",
    "}\n",
    "\n",
    "run[\"parameters\"] = params\n",
    "\n",
    "timestamp = datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    "os.makedirs(f\"weights/{timestamp}\", exist_ok=False)\n",
    "weight_dir = f\"weights/{timestamp}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from optimization.optimization import run_optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0: Loss = 7.035833358764648\n",
      "Iteration 10: Loss = 10.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/spandan/Projects/condensed/hubbard-transformer/.venv/lib/python3.13/site-packages/torch/nn/modules/transformer.py:385: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 20: Loss = 7.761752128601074\n",
      "Iteration 30: Loss = 1.884789228439331\n",
      "Iteration 40: Loss = 4.882350444793701\n",
      "Iteration 50: Loss = 9.657655715942383\n",
      "Iteration 60: Loss = 1.6133136749267578\n",
      "Iteration 70: Loss = 10.0\n",
      "Iteration 80: Loss = 10.0\n",
      "Iteration 90: Loss = 10.0\n",
      "Iteration 100: Loss = 10.0\n",
      "Iteration 110: Loss = 10.0\n",
      "Iteration 120: Loss = 10.0\n",
      "Iteration 130: Loss = 10.0\n",
      "Iteration 140: Loss = 10.0\n",
      "Iteration 150: Loss = 10.0\n",
      "Iteration 160: Loss = 10.0\n",
      "Iteration 170: Loss = 10.0\n",
      "Iteration 180: Loss = 10.0\n",
      "Iteration 190: Loss = 10.0\n",
      "Iteration 200: Loss = 10.0\n",
      "Iteration 210: Loss = 10.0\n",
      "Iteration 220: Loss = 10.0\n",
      "Iteration 230: Loss = 10.0\n",
      "Iteration 240: Loss = 10.0\n",
      "Iteration 250: Loss = 10.0\n",
      "Iteration 260: Loss = 10.0\n",
      "Iteration 270: Loss = 10.0\n",
      "Iteration 280: Loss = 10.0\n",
      "Iteration 290: Loss = 10.0\n",
      "Iteration 300: Loss = 10.0\n",
      "Iteration 310: Loss = 10.0\n",
      "Iteration 320: Loss = 10.0\n",
      "Iteration 330: Loss = 10.0\n",
      "Iteration 340: Loss = 10.0\n",
      "Iteration 350: Loss = 10.0\n",
      "Iteration 360: Loss = 10.0\n",
      "Iteration 370: Loss = 10.0\n",
      "Iteration 380: Loss = 10.0\n",
      "Iteration 390: Loss = 10.0\n",
      "Iteration 400: Loss = 10.0\n",
      "Iteration 410: Loss = 10.0\n",
      "Iteration 420: Loss = 10.0\n",
      "Iteration 430: Loss = 10.0\n",
      "Iteration 440: Loss = 10.0\n",
      "Iteration 450: Loss = 10.0\n",
      "Iteration 460: Loss = 10.0\n",
      "Iteration 470: Loss = 10.0\n",
      "Iteration 480: Loss = 10.0\n",
      "Iteration 490: Loss = 10.0\n",
      "Iteration 500: Loss = 10.0\n",
      "Iteration 510: Loss = 10.0\n",
      "Iteration 520: Loss = 10.0\n",
      "Iteration 530: Loss = 10.0\n",
      "Iteration 540: Loss = 10.0\n",
      "Iteration 550: Loss = 10.0\n",
      "Iteration 560: Loss = 10.0\n",
      "Iteration 570: Loss = 10.0\n",
      "Iteration 580: Loss = 10.0\n",
      "Iteration 590: Loss = 10.0\n",
      "Iteration 600: Loss = 10.0\n",
      "Iteration 610: Loss = 10.0\n",
      "Iteration 620: Loss = 10.0\n",
      "Iteration 630: Loss = 10.0\n",
      "Iteration 640: Loss = 10.0\n",
      "Iteration 650: Loss = 10.0\n",
      "Iteration 660: Loss = 10.0\n",
      "Iteration 670: Loss = 10.0\n",
      "Iteration 680: Loss = 10.0\n",
      "Iteration 690: Loss = 10.0\n",
      "Iteration 700: Loss = 10.0\n",
      "Iteration 710: Loss = 10.0\n",
      "Iteration 720: Loss = 10.0\n",
      "Iteration 730: Loss = 10.0\n",
      "Iteration 740: Loss = 10.0\n",
      "Iteration 750: Loss = 10.0\n",
      "Iteration 760: Loss = 10.0\n",
      "Iteration 770: Loss = 10.0\n",
      "Iteration 780: Loss = 10.0\n",
      "Iteration 790: Loss = 10.0\n",
      "Iteration 800: Loss = 10.0\n",
      "Iteration 810: Loss = 10.0\n",
      "Iteration 820: Loss = 10.0\n",
      "Iteration 830: Loss = 10.0\n",
      "Iteration 840: Loss = 10.0\n",
      "Iteration 850: Loss = 10.0\n",
      "Iteration 860: Loss = 10.0\n",
      "Iteration 870: Loss = 10.0\n",
      "Iteration 880: Loss = 10.0\n",
      "Iteration 890: Loss = 10.0\n",
      "Iteration 900: Loss = 10.0\n",
      "Iteration 910: Loss = 10.0\n",
      "Iteration 920: Loss = 10.0\n",
      "Iteration 930: Loss = 10.0\n",
      "Iteration 940: Loss = 10.0\n",
      "Iteration 950: Loss = 10.0\n",
      "Iteration 960: Loss = 10.0\n",
      "Iteration 970: Loss = 10.0\n",
      "Iteration 980: Loss = 10.0\n",
      "Iteration 990: Loss = 10.0\n",
      "Iteration 1000: Loss = 10.0\n",
      "Iteration 1010: Loss = 10.0\n",
      "Iteration 1020: Loss = 10.0\n",
      "Iteration 1030: Loss = 10.0\n",
      "Iteration 1040: Loss = 10.0\n",
      "Iteration 1050: Loss = 10.0\n",
      "Iteration 1060: Loss = 10.0\n",
      "Iteration 1070: Loss = 10.0\n",
      "Iteration 1080: Loss = 10.0\n",
      "Iteration 1090: Loss = 10.0\n",
      "Iteration 1100: Loss = 10.0\n",
      "Iteration 1110: Loss = 10.0\n",
      "Iteration 1120: Loss = 10.0\n",
      "Iteration 1130: Loss = 10.0\n",
      "Iteration 1140: Loss = 10.0\n",
      "Iteration 1150: Loss = 10.0\n",
      "Iteration 1160: Loss = 10.0\n",
      "Iteration 1170: Loss = 10.0\n",
      "Iteration 1180: Loss = 10.0\n",
      "Iteration 1190: Loss = 10.0\n",
      "Iteration 1200: Loss = 10.0\n",
      "Iteration 1210: Loss = 10.0\n",
      "Iteration 1220: Loss = 10.0\n",
      "Iteration 1230: Loss = 10.0\n",
      "Iteration 1240: Loss = 10.0\n",
      "Iteration 1250: Loss = 10.0\n",
      "Iteration 1260: Loss = 10.0\n",
      "Iteration 1270: Loss = 10.0\n",
      "Iteration 1280: Loss = 10.0\n",
      "Iteration 1290: Loss = 10.0\n",
      "Iteration 1300: Loss = 10.0\n",
      "Iteration 1310: Loss = 10.0\n",
      "Iteration 1320: Loss = 10.0\n",
      "Iteration 1330: Loss = 10.0\n",
      "Iteration 1340: Loss = 10.0\n",
      "Iteration 1350: Loss = 10.0\n",
      "Iteration 1360: Loss = 10.0\n",
      "Iteration 1370: Loss = 10.0\n",
      "Iteration 1380: Loss = 10.0\n",
      "Iteration 1390: Loss = 10.0\n",
      "Iteration 1400: Loss = 10.0\n",
      "Iteration 1410: Loss = 10.0\n",
      "Iteration 1420: Loss = 10.0\n",
      "Iteration 1430: Loss = 10.0\n",
      "Iteration 1440: Loss = 10.0\n",
      "Iteration 1450: Loss = 10.0\n",
      "Iteration 1460: Loss = 10.0\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[27]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m model = \u001b[43mrun_optimization\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m      2\u001b[39m \u001b[43m    \u001b[49m\u001b[43mrun\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrun\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      3\u001b[39m \u001b[43m    \u001b[49m\u001b[43mrun_params\u001b[49m\u001b[43m=\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      4\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      5\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlog_dir\u001b[49m\u001b[43m=\u001b[49m\u001b[43mweight_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      6\u001b[39m \u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Projects/condensed/hubbard-transformer/optimization/optimization.py:74\u001b[39m, in \u001b[36mrun_optimization\u001b[39m\u001b[34m(run, run_params, device, log_dir)\u001b[39m\n\u001b[32m     63\u001b[39m params = torch.tensor(\n\u001b[32m     64\u001b[39m     [\n\u001b[32m     65\u001b[39m         run_params[\u001b[33m\"\u001b[39m\u001b[33mt\u001b[39m\u001b[33m\"\u001b[39m],  \u001b[38;5;66;03m# N\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m     70\u001b[39m     ]\n\u001b[32m     71\u001b[39m )\n\u001b[32m     73\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(run_params[\u001b[33m\"\u001b[39m\u001b[33mepochs\u001b[39m\u001b[33m\"\u001b[39m]):\n\u001b[32m---> \u001b[39m\u001b[32m74\u001b[39m     e_loc_real, e_loc_imag = \u001b[43moptimization_step\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     75\u001b[39m \u001b[43m        \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrun_params\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mbatch_size\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     76\u001b[39m \u001b[43m        \u001b[49m\u001b[43mn_sites\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrun_params\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mn_sites\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     77\u001b[39m \u001b[43m        \u001b[49m\u001b[43mhamiltonian\u001b[49m\u001b[43m=\u001b[49m\u001b[43mham\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     78\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     79\u001b[39m \u001b[43m        \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m=\u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     80\u001b[39m \u001b[43m        \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m=\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     81\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     83\u001b[39m     run[\u001b[33m\"\u001b[39m\u001b[33mloss/epoch/e_loc_real\u001b[39m\u001b[33m\"\u001b[39m].log(e_loc_real.item())\n\u001b[32m     84\u001b[39m     run[\u001b[33m\"\u001b[39m\u001b[33mloss/epoch/e_loc_imag\u001b[39m\u001b[33m\"\u001b[39m].log(e_loc_imag.item())\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Projects/condensed/hubbard-transformer/optimization/optimization.py:36\u001b[39m, in \u001b[36moptimization_step\u001b[39m\u001b[34m(hamiltonian, model, optimizer, params, batch_size, n_sites)\u001b[39m\n\u001b[32m     29\u001b[39m e_loc = model.e_loc(\n\u001b[32m     30\u001b[39m     hamiltonian=hamiltonian,\n\u001b[32m     31\u001b[39m     params=torch.randn(N_PARAMS),  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n\u001b[32m     32\u001b[39m     sampled_states=samples,  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n\u001b[32m     33\u001b[39m )\n\u001b[32m     35\u001b[39m e_loc_real, e_loc_imag = e_loc.real, e_loc.imag\n\u001b[32m---> \u001b[39m\u001b[32m36\u001b[39m \u001b[43me_loc_real\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     37\u001b[39m optimizer.step()\n\u001b[32m     39\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m e_loc_real, e_loc_imag\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Projects/condensed/hubbard-transformer/.venv/lib/python3.13/site-packages/torch/_tensor.py:626\u001b[39m, in \u001b[36mTensor.backward\u001b[39m\u001b[34m(self, gradient, retain_graph, create_graph, inputs)\u001b[39m\n\u001b[32m    616\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    617\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[32m    618\u001b[39m         Tensor.backward,\n\u001b[32m    619\u001b[39m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[32m   (...)\u001b[39m\u001b[32m    624\u001b[39m         inputs=inputs,\n\u001b[32m    625\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m626\u001b[39m \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mautograd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    627\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m=\u001b[49m\u001b[43minputs\u001b[49m\n\u001b[32m    628\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Projects/condensed/hubbard-transformer/.venv/lib/python3.13/site-packages/torch/autograd/__init__.py:347\u001b[39m, in \u001b[36mbackward\u001b[39m\u001b[34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[39m\n\u001b[32m    342\u001b[39m     retain_graph = create_graph\n\u001b[32m    344\u001b[39m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[32m    345\u001b[39m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[32m    346\u001b[39m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m347\u001b[39m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    348\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    349\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    350\u001b[39m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    351\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    352\u001b[39m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    353\u001b[39m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    354\u001b[39m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    355\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Projects/condensed/hubbard-transformer/.venv/lib/python3.13/site-packages/torch/autograd/graph.py:823\u001b[39m, in \u001b[36m_engine_run_backward\u001b[39m\u001b[34m(t_outputs, *args, **kwargs)\u001b[39m\n\u001b[32m    821\u001b[39m     unregister_hooks = _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[32m    822\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m823\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_execution_engine\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[32m    824\u001b[39m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m    825\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[32m    826\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    827\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "model = run_optimization(\n",
    "    run=run,\n",
    "    run_params=params,\n",
    "    device=device,\n",
    "    log_dir=weight_dir,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'model': HubbardWaveFunction(\n",
       "   (embedding): SiteDegreeEmbedding(\n",
       "     (param_embedding): SimpleParamEmbedding()\n",
       "     (token_embedding): OccupationSpinEmbedding()\n",
       "     (position_encoding): PositionEncoding()\n",
       "   )\n",
       "   (encoder_layer): TransformerEncoderLayer(\n",
       "     (self_attn): MultiheadAttention(\n",
       "       (out_proj): NonDynamicallyQuantizableLinear(in_features=32, out_features=32, bias=True)\n",
       "     )\n",
       "     (linear1): Linear(in_features=32, out_features=64, bias=True)\n",
       "     (dropout): Dropout(p=0.1, inplace=False)\n",
       "     (linear2): Linear(in_features=64, out_features=32, bias=True)\n",
       "     (norm1): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
       "     (norm2): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
       "     (dropout1): Dropout(p=0.1, inplace=False)\n",
       "     (dropout2): Dropout(p=0.1, inplace=False)\n",
       "   )\n",
       "   (transformer_encoder): TransformerEncoder(\n",
       "     (layers): ModuleList(\n",
       "       (0-1): 2 x TransformerEncoderLayer(\n",
       "         (self_attn): MultiheadAttention(\n",
       "           (out_proj): NonDynamicallyQuantizableLinear(in_features=32, out_features=32, bias=True)\n",
       "         )\n",
       "         (linear1): Linear(in_features=32, out_features=64, bias=True)\n",
       "         (dropout): Dropout(p=0.1, inplace=False)\n",
       "         (linear2): Linear(in_features=64, out_features=32, bias=True)\n",
       "         (norm1): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
       "         (norm2): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
       "         (dropout1): Dropout(p=0.1, inplace=False)\n",
       "         (dropout2): Dropout(p=0.1, inplace=False)\n",
       "       )\n",
       "     )\n",
       "   )\n",
       "   (deembedding): HubbardDeembedding()\n",
       " )}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cpu'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "str(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[neptune] [info   ] Shutting down background jobs, please wait a moment...\n",
      "[neptune] [info   ] Done!\n",
      "[neptune] [info   ] Waiting for the remaining 42 operations to synchronize with Neptune. Do not kill this process.\n",
      "[neptune] [info   ] All 42 operations synced, thanks for waiting!\n",
      "[neptune] [info   ] Explore the metadata in the Neptune app: https://app.neptune.ai/spagdoon0411/condensed/e/CON-21/metadata\n"
     ]
    }
   ],
   "source": [
    "run.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
