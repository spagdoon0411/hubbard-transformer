{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n",
      "CUDA is available.\n",
      "Changed working dir to /home/spandan/Projects/hubbard-transformer\n"
     ]
    }
   ],
   "source": [
    "from boilerplate import setup_nb\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "device = setup_nb()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [],
   "source": [
    "from embedding import HubbardEmbedding\n",
    "import torch\n",
    "import einops as ein"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_PARAMS = 5\n",
    "TOKEN_DIMS = 32\n",
    "PARAM_DIMS = 16\n",
    "MAX_LEN = 100\n",
    "MIN_OCC = 0\n",
    "MAX_OCC = 10\n",
    "BATCH_SIZE = 32\n",
    "model_dims = PARAM_DIMS + TOKEN_DIMS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding = HubbardEmbedding(TOKEN_DIMS, PARAM_DIMS, N_PARAMS, MAX_LEN).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.2479, -0.0790, -0.5563,  1.0193, -1.5223,  1.0212,  1.1037, -1.4205,\n",
       "         -0.6462, -1.6686,  1.1274, -0.7758,  0.3754,  0.3069, -2.0173, -1.0540,\n",
       "          1.0899,  0.6888,  0.1397, -0.2177,  0.6077,  0.7090, -0.8927, -0.4520,\n",
       "         -1.2204, -1.5151, -0.7829,  1.8253, -0.5316,  1.6677,  0.4271, -0.2675],\n",
       "        [ 0.3799, -1.6730, -1.4425,  1.1204,  0.3681, -0.3839,  1.1935, -1.7573,\n",
       "         -0.8011,  0.6543,  0.3995,  0.1348, -2.3029,  0.3636, -0.0115,  0.2466,\n",
       "          1.9492,  1.7609, -1.7673, -0.3518,  0.1305,  0.3662, -0.4011,  0.9839,\n",
       "         -1.1147,  0.2526,  0.2427, -0.0984,  1.6934, -1.4659, -1.2089, -0.7998],\n",
       "        [ 0.1231,  0.1081,  1.2979,  1.5194, -0.1330,  0.1057, -1.3819,  0.2168,\n",
       "          0.3375,  0.2345,  0.9338, -0.6612,  0.3974,  1.0713,  0.3377, -0.1332,\n",
       "          1.4359,  1.4971,  0.3869, -0.5180, -0.5155,  0.3949,  0.1650, -1.0674,\n",
       "          0.5753,  0.9306, -1.3976,  0.7694, -0.1686,  0.4445, -0.0669, -0.6674],\n",
       "        [-0.6367,  0.1544,  2.2641, -2.1080,  2.6199,  0.0252, -1.7134, -1.0468,\n",
       "          0.3386,  0.3656,  1.4603, -1.3961,  0.1930,  0.8637, -0.5578,  1.3714,\n",
       "         -1.5517,  0.1611,  2.3750, -0.4501,  0.2386,  0.2634, -1.1014,  0.8105,\n",
       "          1.1062, -0.3271,  1.8651, -0.0893,  0.2291, -0.2358, -0.7107,  1.5136],\n",
       "        [-0.4960, -0.4104, -1.2987,  0.7169,  1.1063, -0.2747,  0.2455,  0.1366,\n",
       "          0.7346, -1.3637, -0.6906,  0.3050,  1.2616, -0.3029,  0.3278,  1.3760,\n",
       "         -1.2976,  0.3089,  0.1021,  0.1573, -0.2979,  0.5172, -0.8809,  0.6911,\n",
       "          1.5669, -0.0645, -0.5146,  0.9556, -0.9471, -1.8257,  0.0236,  1.9623]],\n",
       "       device='cuda:0')"
      ]
     },
     "execution_count": 209,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(test_params := torch.randn(N_PARAMS, BATCH_SIZE).to(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[8, 5, 8,  ..., 2, 6, 9],\n",
       "        [0, 1, 4,  ..., 9, 5, 0],\n",
       "        [3, 1, 8,  ..., 7, 5, 9],\n",
       "        ...,\n",
       "        [3, 5, 0,  ..., 4, 1, 4],\n",
       "        [7, 0, 3,  ..., 2, 2, 4],\n",
       "        [5, 9, 0,  ..., 5, 6, 4]], device='cuda:0')"
      ]
     },
     "execution_count": 210,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(test_occupations := torch.randint(MIN_OCC, MAX_OCC, (MAX_LEN, BATCH_SIZE)).to(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-1.3968,  4.1818, -1.1994,  ...,  0.0000,  0.0000,  0.0000],\n",
       "         [-0.8730,  2.6136, -0.7496,  ...,  0.0000,  0.0000,  0.0000],\n",
       "         [-1.3968,  4.1818, -1.1994,  ...,  0.0000,  0.0000,  0.0000],\n",
       "         ...,\n",
       "         [-0.3492,  1.0454, -0.2999,  ...,  0.0000,  0.0000,  0.0000],\n",
       "         [-1.0476,  3.1363, -0.8996,  ...,  0.0000,  0.0000,  0.0000],\n",
       "         [-1.5715,  4.7045, -1.3494,  ...,  0.0000,  0.0000,  0.0000]],\n",
       "\n",
       "        [[-0.0000,  0.0000, -0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "         [-0.1746,  0.5227, -0.1499,  ...,  0.0000,  0.0000,  0.0000],\n",
       "         [-0.6984,  2.0909, -0.5997,  ...,  0.0000,  0.0000,  0.0000],\n",
       "         ...,\n",
       "         [-1.5715,  4.7045, -1.3494,  ...,  0.0000,  0.0000,  0.0000],\n",
       "         [-0.8730,  2.6136, -0.7496,  ...,  0.0000,  0.0000,  0.0000],\n",
       "         [-0.0000,  0.0000, -0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
       "\n",
       "        [[-0.5238,  1.5682, -0.4498,  ...,  0.0000,  0.0000,  0.0000],\n",
       "         [-0.1746,  0.5227, -0.1499,  ...,  0.0000,  0.0000,  0.0000],\n",
       "         [-1.3968,  4.1818, -1.1994,  ...,  0.0000,  0.0000,  0.0000],\n",
       "         ...,\n",
       "         [-1.2222,  3.6591, -1.0495,  ...,  0.0000,  0.0000,  0.0000],\n",
       "         [-0.8730,  2.6136, -0.7496,  ...,  0.0000,  0.0000,  0.0000],\n",
       "         [-1.5715,  4.7045, -1.3494,  ...,  0.0000,  0.0000,  0.0000]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[ 0.0000,  0.0000,  0.0000,  ...,  0.1989,  0.0178,  0.1112],\n",
       "         [ 0.0000,  0.0000,  0.0000,  ...,  0.1746,  0.0156,  0.0976],\n",
       "         [ 0.0000,  0.0000,  0.0000,  ...,  2.0960,  0.1872,  1.1717],\n",
       "         ...,\n",
       "         [ 0.0000,  0.0000,  0.0000,  ...,  0.7179,  0.0641,  0.4013],\n",
       "         [ 0.0000,  0.0000,  0.0000,  ..., -0.1080, -0.0096, -0.0604],\n",
       "         [ 0.0000,  0.0000,  0.0000,  ..., -1.0779, -0.0963, -0.6026]],\n",
       "\n",
       "        [[ 0.0000,  0.0000,  0.0000,  ...,  0.6340,  0.0299,  0.2040],\n",
       "         [ 0.0000,  0.0000,  0.0000,  ..., -0.1538, -0.0073, -0.0495],\n",
       "         [ 0.0000,  0.0000,  0.0000,  ..., -2.2547, -0.1065, -0.7253],\n",
       "         ...,\n",
       "         [ 0.0000,  0.0000,  0.0000,  ...,  0.2348,  0.0111,  0.0755],\n",
       "         [ 0.0000,  0.0000,  0.0000,  ...,  0.7078,  0.0334,  0.2277],\n",
       "         [ 0.0000,  0.0000,  0.0000,  ..., -1.5072, -0.0712, -0.4849]],\n",
       "\n",
       "        [[ 0.0000,  0.0000,  0.0000,  ...,  0.7226,  0.5557,  0.5005],\n",
       "         [ 0.0000,  0.0000,  0.0000,  ...,  0.5980,  0.4598,  0.4142],\n",
       "         [ 0.0000,  0.0000,  0.0000,  ...,  1.8922,  1.4550,  1.3106],\n",
       "         ...,\n",
       "         [ 0.0000,  0.0000,  0.0000,  ...,  2.6600,  2.0454,  1.8424],\n",
       "         [ 0.0000,  0.0000,  0.0000,  ..., -0.0344, -0.0264, -0.0238],\n",
       "         [ 0.0000,  0.0000,  0.0000,  ..., -2.8590, -2.1984, -1.9803]]],\n",
       "       grad_fn=<CopySlices>)"
      ]
     },
     "execution_count": 211,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# seq, batch, embed\n",
    "(logits := embedding(params=test_params, occupations=test_occupations))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([105, 32, 48])\n"
     ]
    }
   ],
   "source": [
    "print(logits.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [],
   "source": [
    "import einops as ein\n",
    "from torch import nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_dims = 16\n",
    "key_dims = 8\n",
    "L = 100\n",
    "batch_dims = 32\n",
    "\n",
    "dtype = torch.complex64\n",
    "\n",
    "# Bring embedding vectors into key space for dot products\n",
    "w_k = nn.Parameter(torch.randn(embed_dims, key_dims, dtype=dtype))\n",
    "w_q = nn.Parameter(torch.randn(embed_dims, key_dims, dtype=dtype))\n",
    "\n",
    "# Reeinterpret embedding vectors as semantic adjustments\n",
    "w_v = nn.Parameter(torch.randn(embed_dims, embed_dims, dtype=dtype))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.6921+0.6522j,  0.5635+1.0998j,  0.8347-0.0733j,  ...,\n",
       "          -0.3270+0.2469j, -0.0692-0.2164j,  0.6165-0.6430j],\n",
       "         [ 1.3129-0.2031j,  0.4259-0.8563j, -0.5041-0.6327j,  ...,\n",
       "           0.7089-0.2514j,  1.8550-0.4296j, -0.8086-0.0404j],\n",
       "         [ 0.1761-0.8038j, -0.6270-0.3766j, -0.9874+1.9265j,  ...,\n",
       "          -0.2693+0.3666j,  0.2185+0.0559j, -0.1989+0.3943j],\n",
       "         ...,\n",
       "         [-1.0189+0.4888j, -0.5706+0.0783j,  0.6153+0.4781j,  ...,\n",
       "           0.0032-0.7725j, -0.2338+0.3859j,  0.6626-0.3513j],\n",
       "         [-0.6344+0.7782j,  0.0228-0.4334j,  0.1028-0.7753j,  ...,\n",
       "          -0.2902-0.1311j,  1.1067+1.0355j, -0.5448-0.9783j],\n",
       "         [ 1.3785+0.1519j,  1.4912-0.4979j, -0.0743+0.1851j,  ...,\n",
       "           0.3881+0.3145j,  0.6313-0.1174j,  0.1679-0.0291j]],\n",
       "\n",
       "        [[ 0.0948+0.0432j, -0.2120+0.3042j, -0.1721+0.3746j,  ...,\n",
       "           1.1749+0.6111j,  0.8997+0.8281j,  0.0615-0.9105j],\n",
       "         [-0.1804+0.1474j, -0.0060+0.7942j, -0.1390-0.9689j,  ...,\n",
       "           0.0434+0.2193j, -0.1191+1.1645j, -0.2068+0.8355j],\n",
       "         [ 0.5704+0.5930j, -0.0431+0.7986j,  0.2325-0.9159j,  ...,\n",
       "           1.5437-0.4314j,  0.6559+0.4507j, -0.2112-0.2140j],\n",
       "         ...,\n",
       "         [ 0.3220-0.1418j, -0.0822-1.0157j,  1.6203+0.6116j,  ...,\n",
       "          -0.1279-0.5326j, -0.5155+0.3895j, -0.0436-0.9085j],\n",
       "         [ 0.5565-0.9387j,  0.7420+0.0241j,  0.6506+0.2404j,  ...,\n",
       "           0.5578-0.3866j,  0.2997+0.4308j,  0.3084-0.6091j],\n",
       "         [-0.3559-0.2665j,  0.8011-0.3387j, -0.3241+0.6035j,  ...,\n",
       "           1.1121+1.7658j, -0.3980+0.2575j,  0.5643-0.3034j]],\n",
       "\n",
       "        [[ 0.6856+1.2154j, -0.4443-0.1635j,  0.0386+0.5379j,  ...,\n",
       "           0.5156-0.4235j, -0.0393-0.3125j, -1.5738+0.2892j],\n",
       "         [ 0.4034+0.1574j,  0.6099-0.5161j, -0.5032+0.3723j,  ...,\n",
       "          -1.0301-1.9144j, -0.7811+0.6640j, -1.2941+0.9042j],\n",
       "         [-1.1150+0.5434j,  0.1267+1.3484j, -0.5698-1.0805j,  ...,\n",
       "           0.3094-1.2712j, -1.3094+0.7669j, -1.1025+0.9556j],\n",
       "         ...,\n",
       "         [ 0.1154+0.0323j, -0.4003+0.2879j, -1.0557+0.0711j,  ...,\n",
       "          -0.0535+0.6562j,  0.8513+0.6820j,  1.3730+0.2888j],\n",
       "         [-1.6001+0.6777j, -0.1717+0.2329j, -0.6103+0.6274j,  ...,\n",
       "          -0.1074-1.1021j, -0.1451-0.7825j, -0.5926-0.0789j],\n",
       "         [-0.7136-0.3867j,  0.3325-0.5680j, -1.1569-1.0168j,  ...,\n",
       "          -1.3535+0.0185j, -0.5427+0.6975j, -1.0194+1.0592j]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[-0.5541-0.2862j, -0.0421-0.5389j, -0.1059-0.3590j,  ...,\n",
       "          -0.8114+0.0288j,  0.0840+0.4288j, -0.6991+0.2372j],\n",
       "         [ 0.4324-0.8324j, -1.0695+0.4900j, -0.2106-0.8791j,  ...,\n",
       "           0.9233-0.0372j,  1.0992+0.2221j,  0.9729+0.9631j],\n",
       "         [ 0.7409+0.2516j, -1.4888+1.4033j,  1.0073+0.0871j,  ...,\n",
       "          -1.3742-0.9687j,  0.2034+0.8662j, -0.7922+0.8049j],\n",
       "         ...,\n",
       "         [ 0.1971+0.5305j, -0.1839-0.1420j, -0.2285-0.1422j,  ...,\n",
       "           1.1449+1.3064j,  0.4423-1.0727j, -0.2392+0.4446j],\n",
       "         [-1.8884-0.5718j, -0.0663-0.1234j,  0.3659+0.5960j,  ...,\n",
       "          -0.0589-0.7388j, -0.0450+0.2641j,  0.6163-0.2240j],\n",
       "         [ 0.2696-0.0505j,  0.5098-0.8747j,  0.9369+0.6228j,  ...,\n",
       "          -0.0705+0.0943j,  1.4833+1.3497j,  0.4061+0.3861j]],\n",
       "\n",
       "        [[-0.8441-0.5926j, -0.2577-0.6585j,  0.8258+0.0560j,  ...,\n",
       "           0.9473-1.0302j, -0.2937+0.5431j,  0.7544-1.0115j],\n",
       "         [ 0.2398-0.4072j,  1.0615-0.0083j,  0.1544-0.3772j,  ...,\n",
       "           0.3578+0.0241j,  0.0513+0.0323j, -0.7392-0.3537j],\n",
       "         [ 0.3886-0.7346j,  0.5693-0.0146j,  1.7327-1.2337j,  ...,\n",
       "           2.1697+0.0953j,  0.4105-1.4535j, -0.1098-0.0871j],\n",
       "         ...,\n",
       "         [-0.2862-0.3431j, -0.9434-0.6602j,  0.1702+0.7147j,  ...,\n",
       "           0.3300-0.2438j,  0.3599+1.3068j, -0.6907-0.9448j],\n",
       "         [ 1.1827+0.3118j,  0.7320+0.1470j,  0.0777-0.3130j,  ...,\n",
       "           0.3725-0.5023j,  0.6514-0.6216j,  0.0495-0.3039j],\n",
       "         [ 0.1152-0.7046j,  0.1305+0.8276j,  1.0510+0.3419j,  ...,\n",
       "          -1.2398-0.5870j,  0.5458+1.2120j,  0.1017+0.2699j]],\n",
       "\n",
       "        [[ 0.4780+0.4273j, -1.0139+0.7631j, -0.1101+0.7567j,  ...,\n",
       "          -0.7748+0.3330j, -0.3026+0.7215j, -0.8509+0.6969j],\n",
       "         [ 0.7093-0.6538j,  0.0620-1.1140j, -0.3242+1.4094j,  ...,\n",
       "           0.3257+0.0700j,  0.1147-0.4010j, -0.0864+1.1618j],\n",
       "         [ 0.7324+0.6802j, -0.4049-1.3999j, -1.1169+0.3183j,  ...,\n",
       "          -0.0994-0.3276j, -0.1248+0.5375j,  0.0988-0.7742j],\n",
       "         ...,\n",
       "         [ 0.4268+0.0026j,  0.6585-0.1103j,  0.5885-0.2303j,  ...,\n",
       "          -0.1197+0.9504j, -0.3683-0.0722j, -1.5717-0.8428j],\n",
       "         [ 0.0983+0.3080j, -0.2191-0.4910j,  0.2779+0.5993j,  ...,\n",
       "           0.6845+0.4885j,  0.5490+1.4210j, -0.5313-0.1752j],\n",
       "         [-0.1070+0.2084j,  1.0186-0.5435j, -0.7412+0.9429j,  ...,\n",
       "           0.4812+1.0030j,  1.2010+0.5809j, -1.7970+0.5926j]]])"
      ]
     },
     "execution_count": 215,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logits = torch.randn(L, batch_dims, embed_dims, dtype=dtype)\n",
    "logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 1.4061e+00-1.7042e+00j, -2.3706e+00+2.1585e-01j,\n",
       "          -4.5957e-01-4.4466e-01j,  ...,\n",
       "          -3.7431e+00+1.0810e+00j, -1.2657e+00+2.7723e+00j,\n",
       "          -1.0430e-01-5.3855e-01j],\n",
       "         [ 6.6107e-01+1.8175e+00j, -1.6230e+00+7.4774e-02j,\n",
       "           1.3100e+00+1.0855e+00j,  ...,\n",
       "           1.6108e+00-3.0223e+00j, -7.1808e-01+7.9994e-01j,\n",
       "           6.6655e-02+1.7232e+00j],\n",
       "         [ 1.4068e+00-1.7035e+00j, -2.3695e+00+2.1618e-01j,\n",
       "          -4.5895e-01-4.4513e-01j,  ...,\n",
       "          -3.7410e+00+1.0803e+00j, -1.2662e+00+2.7721e+00j,\n",
       "          -1.0487e-01-5.3756e-01j],\n",
       "         ...,\n",
       "         [ 6.6330e-01-1.9276e+00j, -2.2545e+00-1.6822e+00j,\n",
       "           1.0419e+00-1.9083e+00j,  ...,\n",
       "          -5.4447e+00+1.7153e+00j,  7.6035e-02+1.7367e+00j,\n",
       "           1.1600e+00+1.8771e+00j],\n",
       "         [-4.2091e+00+4.1052e+00j, -2.0763e+00+6.0765e-01j,\n",
       "          -1.3160e+00+3.4147e-01j,  ...,\n",
       "           5.2648e+00+6.3590e+00j,  2.9326e+00+2.4325e+00j,\n",
       "           1.0283e+00+4.2742e-01j],\n",
       "         [ 1.4071e+00-1.7045e+00j, -2.3682e+00+2.2119e-01j,\n",
       "          -4.6321e-01-4.3939e-01j,  ...,\n",
       "          -3.7218e+00+1.0750e+00j, -1.2616e+00+2.7650e+00j,\n",
       "          -1.0192e-01-5.3448e-01j]],\n",
       "\n",
       "        [[-8.2061e+00+2.2988e+00j, -1.5747e-01-3.6558e+00j,\n",
       "          -6.9367e-01-2.4258e+00j,  ...,\n",
       "          -4.3255e+00-1.1308e+00j,  2.6868e+00-4.4065e-01j,\n",
       "           2.4071e+00-8.1501e+00j],\n",
       "         [ 3.6500e+00+7.0181e+00j,  2.7586e+00+4.2644e-01j,\n",
       "          -5.4595e+00+4.5725e+00j,  ...,\n",
       "           6.4082e-01-4.1525e-01j,  3.7669e+00-3.2885e+00j,\n",
       "           1.0215e-01-2.7095e+00j],\n",
       "         [ 7.6208e-01-2.2301e-01j,  1.7342e-01+3.2251e+00j,\n",
       "          -8.3229e-01+5.3238e-01j,  ...,\n",
       "           1.6093e-01+1.4986e+00j, -4.9350e-01-1.3927e+00j,\n",
       "          -2.8461e+00+9.6451e-01j],\n",
       "         ...,\n",
       "         [-1.0756e+00-1.4195e+00j, -8.6621e-01+6.0131e+00j,\n",
       "          -4.1727e-02-1.7334e-01j,  ...,\n",
       "           5.5773e-01+2.2303e+00j, -1.9758e+00-1.2447e+00j,\n",
       "          -3.7862e+00+1.6979e+00j],\n",
       "         [ 3.2703e+00-3.6330e+00j, -2.2016e+00-2.8347e+00j,\n",
       "           3.1033e-01+1.0413e+00j,  ...,\n",
       "          -2.3126e-01-1.9661e+00j, -3.9484e+00+3.8507e+00j,\n",
       "          -4.4765e-02+2.8066e+00j],\n",
       "         [-3.6059e+00+8.3292e-01j,  3.2138e+00+1.6245e+00j,\n",
       "          -2.4192e+00-4.8138e+00j,  ...,\n",
       "          -3.8773e+00+2.4473e+00j,  1.0529e+00-2.6474e+00j,\n",
       "          -1.5591e+00-3.0363e+00j]],\n",
       "\n",
       "        [[ 9.9680e-01-3.0162e+00j,  1.6763e-01-4.0037e+00j,\n",
       "          -1.7017e+00-1.8598e+00j,  ...,\n",
       "          -2.8275e+00+8.0807e-01j,  1.9350e+00+2.6923e+00j,\n",
       "           2.4339e+00-2.0310e+00j],\n",
       "         [ 9.9680e-01-3.0162e+00j,  1.6763e-01-4.0037e+00j,\n",
       "          -1.7017e+00-1.8598e+00j,  ...,\n",
       "          -2.8275e+00+8.0807e-01j,  1.9350e+00+2.6923e+00j,\n",
       "           2.4339e+00-2.0310e+00j],\n",
       "         [ 1.2292e+00-2.1182e+00j,  4.3259e-01-3.3856e+00j,\n",
       "          -1.0708e+00-1.8248e+00j,  ...,\n",
       "          -2.5905e+00+6.8825e-03j,  1.8681e+00+2.1755e+00j,\n",
       "           3.0230e+00-1.3079e+00j],\n",
       "         ...,\n",
       "         [ 5.4760e-01-2.7372e+00j, -1.2554e+00-3.9229e+00j,\n",
       "           3.1625e+00-1.8280e+00j,  ...,\n",
       "          -2.5354e+00+3.6320e+00j,  5.8162e-01+1.9354e+00j,\n",
       "          -2.8710e+00+4.1496e+00j],\n",
       "         [ 9.9680e-01-3.0158e+00j,  1.6798e-01-4.0032e+00j,\n",
       "          -1.7013e+00-1.8594e+00j,  ...,\n",
       "          -2.8269e+00+8.0775e-01j,  1.9344e+00+2.6921e+00j,\n",
       "           2.4334e+00-2.0302e+00j],\n",
       "         [ 3.6162e+00+2.5222e+00j,  4.3892e-01+8.9517e-01j,\n",
       "          -1.6204e+00+9.6487e-01j,  ...,\n",
       "           2.1940e+00+3.2415e+00j, -2.5460e+00-1.1454e+00j,\n",
       "          -3.5657e+00-8.7300e-03j]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[ 1.1113e+00-5.2586e-01j, -1.8516e+00-3.1046e+00j,\n",
       "           3.1111e+00-5.6850e-01j,  ...,\n",
       "          -5.6003e-01+8.6907e-01j, -1.2971e+00+4.7159e+00j,\n",
       "           1.9785e+00+4.2097e+00j],\n",
       "         [-1.9430e+00-3.8246e-01j,  2.9392e+00-4.2209e+00j,\n",
       "           1.0863e+00+4.4960e-01j,  ...,\n",
       "          -7.5500e-01-1.0415e+00j,  1.2052e+00+4.0935e+00j,\n",
       "           3.9551e+00+2.3324e+00j],\n",
       "         [ 8.8443e-01+2.1756e+00j,  1.8820e+00-1.5041e+00j,\n",
       "          -2.4392e+00+8.0466e-01j,  ...,\n",
       "          -1.0760e+00-2.4169e+00j,  6.1408e-01-8.6991e-01j,\n",
       "           3.3684e+00-3.0000e+00j],\n",
       "         ...,\n",
       "         [ 1.6528e+00-2.2220e-01j, -2.3122e+00-1.2117e+00j,\n",
       "          -2.6813e+00+1.7721e+00j,  ...,\n",
       "          -2.4251e+00-2.3982e+00j,  2.4726e+00-2.2347e+00j,\n",
       "           6.2316e+00-4.2802e+00j],\n",
       "         [-2.6758e-01+4.7687e+00j,  1.0475e+00-8.3448e-01j,\n",
       "          -8.4039e-01-2.2916e+00j,  ...,\n",
       "          -1.1392e-01+2.7705e+00j, -4.1882e-01+2.4549e+00j,\n",
       "           1.5222e-02+1.0548e+00j],\n",
       "         [-4.2908e+00+4.1106e-01j, -1.3909e+00+9.9268e-01j,\n",
       "           1.3733e+00-3.8112e+00j,  ...,\n",
       "          -6.2008e-01+6.2218e+00j,  2.4491e+00+1.5400e+00j,\n",
       "           1.6938e+00-1.8151e+00j]],\n",
       "\n",
       "        [[ 1.8613e+00+2.9783e+00j, -9.5436e-01+2.0810e+00j,\n",
       "          -2.7749e+00-5.7043e-01j,  ...,\n",
       "           3.2726e+00-3.0278e+00j, -1.3957e+00-3.3111e+00j,\n",
       "          -2.6904e+00-8.4457e-01j],\n",
       "         [ 3.8661e+00+4.7021e+00j, -1.5235e-01-2.5187e+00j,\n",
       "          -3.9950e+00-1.9869e+00j,  ...,\n",
       "           6.2437e+00-1.8282e+00j, -9.3116e-02-1.7371e+00j,\n",
       "          -1.9036e+00-2.4400e+00j],\n",
       "         [ 1.7705e+00+1.0789e+00j, -7.7524e+00+3.8538e+00j,\n",
       "           1.1752e+00+5.9128e-02j,  ...,\n",
       "          -1.8184e+00-4.6784e+00j,  6.0695e-04-3.1227e+00j,\n",
       "          -1.9241e+00+2.1273e+00j],\n",
       "         ...,\n",
       "         [-9.9539e-01-8.1321e-01j,  2.6480e+00-2.6779e+00j,\n",
       "           3.8487e-01-5.2092e+00j,  ...,\n",
       "          -2.4633e+00+8.2400e-01j,  2.5065e+00+4.6591e+00j,\n",
       "           3.1844e+00+2.3557e+00j],\n",
       "         [ 8.7799e-01-1.0822e+00j,  1.9503e+00-1.1233e+00j,\n",
       "          -8.9811e-01+1.6283e+00j,  ...,\n",
       "          -1.3711e+00-3.6561e-01j, -3.2224e+00+1.9963e+00j,\n",
       "          -2.4820e+00+1.2180e+00j],\n",
       "         [ 2.5416e-01+4.0865e+00j,  4.0771e+00-4.1271e+00j,\n",
       "          -4.0540e+00+8.0020e-01j,  ...,\n",
       "           6.3686e-02-3.0292e+00j,  3.5227e+00+3.1317e+00j,\n",
       "           3.3911e+00+1.4769e+00j]],\n",
       "\n",
       "        [[-1.8772e+00-2.2336e-01j,  2.3917e+00+2.3436e+00j,\n",
       "           6.5858e-01+2.5318e+00j,  ...,\n",
       "           4.5740e+00+3.0660e+00j, -1.5696e+00+9.4093e-01j,\n",
       "          -2.0082e+00-5.6825e-01j],\n",
       "         [ 1.4393e-01-1.9867e+00j,  2.4316e+00-1.5883e+00j,\n",
       "           2.2046e-01+1.5252e+00j,  ...,\n",
       "           7.0005e-01-2.7320e+00j, -1.9702e+00+1.5737e+00j,\n",
       "          -2.2359e-01+3.7722e+00j],\n",
       "         [-5.2549e-01-4.3823e+00j,  2.8647e+00-1.0307e-01j,\n",
       "           6.9611e-01+2.3417e+00j,  ...,\n",
       "           1.3477e+00-2.0135e+00j, -1.2803e+00+2.3191e+00j,\n",
       "           3.6597e-02+6.0950e+00j],\n",
       "         ...,\n",
       "         [-4.7714e-01+2.0996e+00j,  5.7990e-01-3.7660e+00j,\n",
       "          -2.1551e+00-1.0720e+00j,  ...,\n",
       "          -1.0234e+00-8.6705e-02j,  6.6337e-01+1.7353e+00j,\n",
       "          -1.2870e-01+1.2645e+00j],\n",
       "         [ 5.3358e+00+3.9626e-01j,  3.5978e+00-6.6545e-01j,\n",
       "          -2.6086e-01+5.1101e+00j,  ...,\n",
       "           1.3284e+00+1.9858e-02j, -1.4267e+00-1.8860e+00j,\n",
       "           1.3529e+00+3.3071e-01j],\n",
       "         [ 5.3345e+00+3.9733e-01j,  3.5989e+00-6.6580e-01j,\n",
       "          -2.6014e-01+5.1085e+00j,  ...,\n",
       "           1.3308e+00+1.6978e-02j, -1.4284e+00-1.8864e+00j,\n",
       "           1.3514e+00+3.3176e-01j]]], grad_fn=<UnsafeViewBackward0>)"
      ]
     },
     "execution_count": 216,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q = torch.matmul(logits, w_q)\n",
    "k = torch.matmul(logits, w_k)\n",
    "v = torch.matmul(logits, w_v)\n",
    "\n",
    "_q = ein.rearrange(q, \"s b k -> b s k\")\n",
    "_kH = ein.rearrange(k, \"s b k -> b k s\").conj()\n",
    "_v = ein.rearrange(v, \"s b k -> b s k\")\n",
    "similarities = _q @ _kH\n",
    "\n",
    "# TODO: this causes a reallocation\n",
    "similarities = torch.softmax(torch.abs(similarities), dim=-1).to(dtype=dtype)\n",
    "adjustments = similarities @ _v\n",
    "adjustments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def complex_attention_update(logits, w_q, w_k, w_v) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    logits: (seq, batch, embed)\n",
    "    w_q: (embed, key, head)\n",
    "    w_k: (embed, key, head)\n",
    "    w_v: (embed, embed, head)\n",
    "\n",
    "    # TODO: rotary embedding?\n",
    "\n",
    "    Returns: (seq, batch, embed) - The updated logits after passing through\n",
    "    multi-headed attention\n",
    "    \"\"\"\n",
    "\n",
    "    dtype = torch.complex64\n",
    "\n",
    "    # TODO: justify that this is right. Translate to einops?\n",
    "    # Weights are in the form (heads, embed, out) where out \\in {key, embed}\n",
    "    # (seq, batch, embed) -> (seq, 1, batch, embed)\n",
    "    # (head, embed, out) ->  (1, head, embed, out)\n",
    "    # (seq, 1, batch, embed) @ (1, head, embed, key) = (seq, head, batch, key)\n",
    "\n",
    "    # Create a new \"head\" dimension\n",
    "    logits = logits.unsqueeze(1)\n",
    "    w_q = w_q.unsqueeze(0)\n",
    "    w_k = w_k.unsqueeze(0)\n",
    "    w_v = w_v.unsqueeze(0)\n",
    "\n",
    "    q = torch.matmul(logits, w_q)\n",
    "    k = torch.matmul(logits, w_k)\n",
    "    v = torch.matmul(logits, w_v)\n",
    "\n",
    "    # Batched matmul between A and B will\n",
    "    # Broadcasting in matmul will only look at the batch dimensions\n",
    "    # and NOT the sequence dimensions\n",
    "\n",
    "    # note that vec-mat matmul is inherently batched.\n",
    "    # Pytorch's version is simply an extension of the regular matmul \"batching\"\n",
    "\n",
    "    _q = ein.rearrange(q, \"s h b k -> b h s k\")\n",
    "    _kH = ein.rearrange(k, \"s h b k -> b h k s\").conj()\n",
    "    _v = ein.rearrange(v, \"s h b e -> b h s e\")\n",
    "\n",
    "    # (batch, head, seq, seq) @ (batch, head, sequence, embed)\n",
    "    # TODO: them matrix dimensions should be clear from here.\n",
    "    pattern = torch.softmax((_q @ _kH).abs(), dim=-1).to(dtype=dtype)\n",
    "    updates = torch.sum(ein.rearrange(pattern @ _v, \"b h s e -> s b e h\"), dim=-1)\n",
    "    return logits.squeeze(1) + updates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([100, 32, 16])\n",
      "torch.Size([100, 32, 16])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([100, 32, 16])"
      ]
     },
     "execution_count": 225,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "complex_attention_update(logits, w_q, w_k, w_v).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tqs2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
